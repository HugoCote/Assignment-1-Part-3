
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{dev1num3}
    
    
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=0.2in,bmargin=0.2in,lmargin=0.2in,rmargin=0.2in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Assigment 1, part3}\label{assigment-1-part3}

Link to github : https://github.com/HugoCote/Assignment-1-Part-3/

\subsubsection{Members of the team :}\label{members-of-the-team}

\begin{itemize}
\tightlist
\item
  Srinivas Venkattaramanujam\\
\item
  Jean-Philippe Gagnon Fleury\\
\item
  Ahmadreza Godarzvandchegini\\
\item
  Hugo Côté
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} deep learning library}
        \PY{k+kn}{import} \PY{n+nn}{torch}
        \PY{k+kn}{import} \PY{n+nn}{torchvision}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{init} \PY{k}{as} \PY{n+nn}{init} \PY{c+c1}{\PYZsh{} to initialize model}
        \PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k}{import} \PY{n}{Dataset}\PY{p}{,} \PY{n}{DataLoader}
        \PY{k+kn}{from} \PY{n+nn}{torchvision} \PY{k}{import} \PY{n}{transforms}\PY{p}{,} \PY{n}{utils}
        
        \PY{c+c1}{\PYZsh{} we use torch.cuda.Event(enable\PYZus{}timing=True) to measure time}
        \PY{c+c1}{\PYZsh{} if you don\PYZsq{}t have cuda, you can use instead :}
        \PY{c+c1}{\PYZsh{} from timeit import default\PYZus{}timer as timer}
        \PY{c+c1}{\PYZsh{} import time}
        
        \PY{k+kn}{import} \PY{n+nn}{collections} \PY{c+c1}{\PYZsh{} for ordered\PYZus{}dictionnary}
        \PY{k+kn}{import} \PY{n+nn}{copy}        \PY{c+c1}{\PYZsh{} for copy.deepcopy( ... )}
        \PY{k+kn}{import} \PY{n+nn}{math}        \PY{c+c1}{\PYZsh{} for ceiling function}
        
        \PY{c+c1}{\PYZsh{} to display plot}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt} 
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}              
        
        \PY{c+c1}{\PYZsh{} to import data}
        \PY{k+kn}{from} \PY{n+nn}{\PYZus{}\PYZus{}future\PYZus{}\PYZus{}} \PY{k}{import} \PY{n}{print\PYZus{}function}\PY{p}{,} \PY{n}{division}
        \PY{k+kn}{import} \PY{n+nn}{os}
        \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{Image}           \PY{c+c1}{\PYZsh{} }
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{from} \PY{n+nn}{skimage} \PY{k}{import} \PY{n}{io}\PY{p}{,} \PY{n}{transform} 
        
        \PY{k+kn}{import} \PY{n+nn}{datetime}                 \PY{c+c1}{\PYZsh{} to format time in strings}
        
        \PY{k+kn}{import} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{c+c1}{\PYZsh{} to display .png inside the notebook}
\end{Verbatim}

    \paragraph{Some cells could require a long time to
evaluate,}\label{some-cells-could-require-a-long-time-to-evaluate}

to warn the user that the evaluation of one such cell is completed, it
outputs a sound.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Audio}
        \PY{n}{wave} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{l+m+mf}{1.5}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{o}{*}\PY{l+m+mi}{400}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{10000}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{10000}\PY{p}{)} 
        \PY{c+c1}{\PYZsh{} the following command line produces sound and is used after cells that }
        \PY{c+c1}{\PYZsh{} require more time to execute :}
        \PY{n}{Audio}\PY{p}{(}\PY{n}{wave}\PY{p}{,} \PY{n}{rate}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,} \PY{n}{autoplay}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:} <IPython.lib.display.Audio object>
\end{Verbatim}
            
    \paragraph{If you did not liked that sound, you should disable
it}\label{if-you-did-not-liked-that-sound-you-should-disable-it}

By setting want\_lound\_warning to false.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{want\PYZus{}lound\PYZus{}warning} \PY{o}{=} \PY{k+kc}{False}
\end{Verbatim}

    \paragraph{To perfrom the hyper-parameters search, we use the following
library
:}\label{to-perfrom-the-hyper-parameters-search-we-use-the-following-library}

It can be installed with the following command :\\
- pip install sobol\_seq

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} !pip install sobol\PYZus{}seq}
        \PY{k+kn}{import} \PY{n+nn}{sobol\PYZus{}seq}
\end{Verbatim}

    \subsubsection{Import the data used for training and
validation}\label{import-the-data-used-for-training-and-validation}

And instantiate two datasets, one with and one without data
augmentation.\\
We tried different intensity of data augmentation. We named them :\\
- high\\
- medium\\
- medium-low\\
- low - normal (no data augmentation)

The differents transformations used for augmentation are : - Random
horizontal flip\\
- Random resize and crop - Randomly converting to grayscale - Random
rotation

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{random\PYZus{}seed}\PY{o}{=} \PY{l+m+mi}{2019} \PY{c+c1}{\PYZsh{} for reproducibility }
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{8}
        \PY{n}{validation\PYZus{}split} \PY{o}{=} \PY{o}{.}\PY{l+m+mi}{10} \PY{c+c1}{\PYZsh{} fraction of samples that will belong to the validation dataset}
        \PY{n}{shuffle\PYZus{}dataset} \PY{o}{=} \PY{k+kc}{True}
        \PY{n}{num\PYZus{}workers} \PY{o}{=} \PY{l+m+mi}{0}        \PY{c+c1}{\PYZsh{} dataloader issues with numworkers \PYZgt{} 0}
        
        \PY{c+c1}{\PYZsh{} used to scale tensor from [0 to 1] to [0 to 255]}
        \PY{c+c1}{\PYZsh{} Without this, with the hyper\PYZhy{}parameters tested, the models stay at 50\PYZpc{} accuracy}
        \PY{k}{def} \PY{n+nf}{multby255} \PY{p}{(}\PY{n}{pic}\PY{p}{)} \PY{p}{:}
            \PY{k}{return} \PY{n}{pic}\PY{o}{.}\PY{n}{mul}\PY{p}{(}\PY{l+m+mi}{255}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} setting up data loader directory for training and validation}
        \PY{n}{root} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data\PYZus{}catdogs/trainset/}\PY{l+s+s1}{\PYZsq{}}
        
        \PY{c+c1}{\PYZsh{} different ways to augment the data}
        \PY{n}{data\PYZus{}transforms} \PY{o}{=} \PY{p}{\PYZob{}}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{high}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomGrayscale}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.15}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomHorizontalFlip}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomResizedCrop}\PY{p}{(}\PY{l+m+mi}{90}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.80}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{,} \PY{n}{ratio}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.75}\PY{p}{,} \PY{l+m+mf}{1.25}\PY{p}{)}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{CenterCrop}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{Lambda}\PY{p}{(}\PY{n}{multby255}\PY{p}{)}
            \PY{p}{]}\PY{p}{)}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{medium}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomGrayscale}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.15}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomHorizontalFlip}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomResizedCrop}\PY{p}{(}\PY{l+m+mi}{80}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.85}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{,} \PY{n}{ratio}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mf}{1.2}\PY{p}{)}\PY{p}{,} \PY{n}{interpolation}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{CenterCrop}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{Lambda}\PY{p}{(}\PY{n}{multby255}\PY{p}{)}
            \PY{p}{]}\PY{p}{)}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{medium\PYZhy{}low}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomGrayscale}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomChoice}\PY{p}{(}\PY{p}{[}
                    \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomHorizontalFlip}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{)}\PY{p}{,}
                    \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomRotation}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{)}\PY{p}{,}
                    \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomResizedCrop}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.95}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}\PY{p}{,} \PY{n}{ratio}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.95}\PY{p}{,} \PY{l+m+mf}{1.05}\PY{p}{)}\PY{p}{)}
                \PY{p}{]}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{Lambda}\PY{p}{(}\PY{n}{multby255}\PY{p}{)}
            \PY{p}{]}\PY{p}{)}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{low}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomGrayscale}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.15}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomHorizontalFlip}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{Lambda}\PY{p}{(}\PY{n}{multby255}\PY{p}{)}
            \PY{p}{]}\PY{p}{)}\PY{p}{,}
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                \PY{n}{transforms}\PY{o}{.}\PY{n}{Lambda}\PY{p}{(}\PY{n}{multby255}\PY{p}{)}
            \PY{p}{]}\PY{p}{)}
        \PY{p}{\PYZcb{}}
        
        \PY{c+c1}{\PYZsh{} to be able to train and valide on both the original and the augmented dataset}
        \PY{n}{train\PYZus{}dataset\PYZus{}augm} \PY{o}{=} \PY{n}{torchvision}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{ImageFolder}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{n}{root}\PY{p}{,}\PY{n}{transform}\PY{o}{=}\PY{n}{data\PYZus{}transforms}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{medium\PYZhy{}low}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
        \PY{n}{train\PYZus{}dataset\PYZus{}norm} \PY{o}{=} \PY{n}{torchvision}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{ImageFolder}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{n}{root}\PY{p}{,}\PY{n}{transform}\PY{o}{=}\PY{n}{data\PYZus{}transforms}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{normal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Creating data indices for training and validation splits:}
        \PY{n}{train\PYZus{}dataset\PYZus{}size} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}augm}\PY{p}{)}
        \PY{n}{indices} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}size}\PY{p}{)}\PY{p}{)}
        \PY{n}{split} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{n}{validation\PYZus{}split} \PY{o}{*} \PY{n}{train\PYZus{}dataset\PYZus{}size}\PY{p}{)}\PY{p}{)}
        \PY{k}{if} \PY{n}{shuffle\PYZus{}dataset} \PY{p}{:}
            \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{random\PYZus{}seed}\PY{p}{)}
            \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{indices}\PY{p}{)}
        \PY{n}{train\PYZus{}indices}\PY{p}{,} \PY{n}{val\PYZus{}indices} \PY{o}{=} \PY{n}{indices}\PY{p}{[}\PY{n}{split}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{indices}\PY{p}{[}\PY{p}{:}\PY{n}{split}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Creating data samplers:}
        \PY{n}{train\PYZus{}sampler} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{SubsetRandomSampler}\PY{p}{(}\PY{n}{train\PYZus{}indices}\PY{p}{)}
        \PY{n}{valid\PYZus{}sampler} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{SubsetRandomSampler}\PY{p}{(}\PY{n}{val\PYZus{}indices}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} The following code show how to instantiate dataloader for the training and validation datasets.}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}
        \PY{n}{train\PYZus{}norm\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
        \PY{n}{train\PYZus{}augm\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}augm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
        \PY{n}{valid\PYZus{}norm\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
        \PY{n}{valid\PYZus{}augm\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}augm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
\end{Verbatim}

    \paragraph{Compute and display the size of each
dataset}\label{compute-and-display-the-size-of-each-dataset}

We made a 10\% split :\\
- 10\% of labelled pictures belong to the validation dataset\\
- 90\% of labelled pictures belong to the training dataset

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{dummy\PYZus{}train\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}augm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{dummy\PYZus{}valid\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}augm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{train\PYZus{}dataset\PYZus{}size}  \PY{o}{=} \PY{n}{dummy\PYZus{}train\PYZus{}loader}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n}{valid\PYZus{}dataset\PYZus{}size}   \PY{o}{=} \PY{n}{dummy\PYZus{}valid\PYZus{}loader}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{training   dataset size : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{train\PYZus{}dataset\PYZus{}size}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{validation dataset size : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{valid\PYZus{}dataset\PYZus{}size}\PY{p}{)}
        \PY{k}{del} \PY{n}{dummy\PYZus{}train\PYZus{}loader} 
        \PY{k}{del} \PY{n}{dummy\PYZus{}valid\PYZus{}loader} 
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
training   dataset size :  17999
validation dataset size :  1999

    \end{Verbatim}

    \subsubsection{Display some samples}\label{display-some-samples}

Using data augmentation. This is usefull to confirm that the augmented
data preserve enough informations about the original data to be relevant
for training i.e. pictures are not so modified that a human is not able
to label them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{8}
        \PY{n}{pict\PYZus{}n\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
        \PY{n}{pict\PYZus{}a\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}augm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} function to show an image}
        \PY{k}{def} \PY{n+nf}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{:}
            \PY{n}{npimg} \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}  \PY{o}{/} \PY{l+m+mi}{255}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{npimg}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{images}\PY{p}{,} \PY{n}{labels}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{pict\PYZus{}a\PYZus{}loader}\PY{p}{)} \PY{p}{:}
            \PY{k}{if} \PY{n}{i} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0} \PY{p}{:} \PY{k}{break}
            \PY{c+c1}{\PYZsh{} show images}
            \PY{n}{imshow}\PY{p}{(}\PY{n}{torchvision}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{images} \PY{p}{)}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} print labels}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}5s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{labels}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{min}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{n}{sample\PYZus{}image} \PY{o}{=} \PY{n}{images}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        
        \PY{n}{imshow}\PY{p}{(}\PY{n}{torchvision}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(} \PY{n}{sample\PYZus{}image} \PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
    0     1     1     0     0     1     1     1

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_13_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Set the device}\label{set-the-device}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda:0}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
cuda:0

    \end{Verbatim}

    \subsection{The models}\label{the-models}

Here, we define the models that we will be using for the remaining of
the notebook. They are all described.

    Architecture of Classifier inspired by :
https://github.com/MaximumEntropy/welcome\_tutorials/tree/pytorch/pytorch

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{k}{class} \PY{n+nc}{Classifier5}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Classifier5 :}
         \PY{l+s+sd}{    7 Convolutional layers using stride=1, no dilatation and padding to assure same convolution, all having :}
         \PY{l+s+sd}{        \PYZhy{} kernel of size 3 (first 3 layers) or 5 (last 4 layers)}
         \PY{l+s+sd}{        \PYZhy{} double the number of feature maps received from the previous layer}
         \PY{l+s+sd}{        \PYZhy{} followed by ReLU non\PYZhy{}linearity }
         \PY{l+s+sd}{        \PYZhy{} and non\PYZhy{}overlapping max pooling with kernel of size 2 }
         \PY{l+s+sd}{        \PYZhy{} which means that each layer (made of those 3 steps) :}
         \PY{l+s+sd}{            \PYZhy{} receive as input n  feature maps of size 2m x 2m}
         \PY{l+s+sd}{            \PYZhy{} return  as outpu 2n feature maps of size  m x  m}
         \PY{l+s+sd}{    With the exeption of :}
         \PY{l+s+sd}{        \PYZhy{} the 4th layer does not have a max pooling}
         \PY{l+s+sd}{        \PYZhy{} the last layer does not have ReLU non\PYZhy{}linearity}
         \PY{l+s+sd}{    After the convolutional part of the model, the original 3x64x64 input picture is now a 512x1x1 vector.}
         \PY{l+s+sd}{    The 7 conv. layers are followed by one fully connected linear layer}
         \PY{l+s+sd}{    For the output of this model to be seen as a probabilie dist., it has to be fed to a F.softmax(...,dim=\PYZhy{}1)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self} \PY{p}{)}\PY{p}{:}
                 
                 \PY{n}{kernel\PYZus{}sz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
                 \PY{n}{pad} \PY{o}{=} \PY{n}{kernel\PYZus{}sz} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2} 
                 
                 \PY{n+nb}{super}\PY{p}{(}\PY{n}{Classifier5}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
                     \PY{c+c1}{\PYZsh{} Layer, input size = 64\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 2, input size = 32\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 3, input size = 16\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} Layer 4, input size = 8\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} Layer 5, input size = 8\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 6, input size = 4\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 7, input size = 2\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{c+c1}{\PYZsh{} nn.ReLU(),}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}           
                 \PY{p}{)}
                 \PY{c+c1}{\PYZsh{} }
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{512}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1b}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 \PY{k}{return} \PY{n}{x}
             
             \PY{k}{def} \PY{n+nf}{to\PYZus{}string}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{n}{depth\PYZus{}to\PYZus{}string} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The depth of this model is fixed to 8}\PY{l+s+s2}{\PYZdq{}}
                 \PY{k}{return} \PY{n}{depth\PYZus{}to\PYZus{}string} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}doc\PYZus{}\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{class} \PY{n+nc}{Classifier5d}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Classifier5d, old version of Classifier5}
         \PY{l+s+sd}{    7 Convolutional layers using stride=1, no dilatation and padding to assure same convolution, all having :}
         \PY{l+s+sd}{        \PYZhy{} kernel of size 3 (first 3 layers) or 5 (last 4 layers)}
         \PY{l+s+sd}{        \PYZhy{} double the number of feature maps received from the previous layer}
         \PY{l+s+sd}{        \PYZhy{} followed by ReLU non\PYZhy{}linearity }
         \PY{l+s+sd}{        \PYZhy{} and non\PYZhy{}overlapping max pooling with kernel of size 2 }
         \PY{l+s+sd}{        \PYZhy{} which means that each layer (made of those 3 steps) :}
         \PY{l+s+sd}{            \PYZhy{} receive as input n  feature maps of size 2m x 2m}
         \PY{l+s+sd}{            \PYZhy{} return  as outpu 2n feature maps of size  m x  m}
         \PY{l+s+sd}{    With the exeption of :}
         \PY{l+s+sd}{        \PYZhy{} the 4th layer does not have a max pooling}
         \PY{l+s+sd}{        \PYZhy{} the last layer has tanh non\PYZhy{}linearity}
         \PY{l+s+sd}{    After the convolutional part of the model, the original 3x64x64 input picture is now a 512x1x1 vector.}
         \PY{l+s+sd}{    The 7 conv. layers are followed by one fully connected layer ending with softmax non\PYZhy{}linearity.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self} \PY{p}{)}\PY{p}{:}
                 
                 \PY{n}{kernel\PYZus{}sz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
                 \PY{n}{pad} \PY{o}{=} \PY{n}{kernel\PYZus{}sz} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2} 
                 \PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{7}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0} 
                 
                 \PY{n+nb}{super}\PY{p}{(}\PY{n}{Classifier5d}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
                     \PY{c+c1}{\PYZsh{} Layer, input size = 64\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 2, input size = 32\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 3, input size = 16\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} Layer 4, input size = 8\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} Layer 5, input size = 8\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 6, input size = 4\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 7, input size = 2\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
                 \PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{512}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{512}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1b}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{k}{return} \PY{n}{x}
             
             \PY{k}{def} \PY{n+nf}{to\PYZus{}string}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{n}{depth\PYZus{}to\PYZus{}string} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The depth of this model is fixed to 8}\PY{l+s+s2}{\PYZdq{}}
                 \PY{k}{return} \PY{n}{depth\PYZus{}to\PYZus{}string} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}doc\PYZus{}\PYZus{}}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k}{class} \PY{n+nc}{Classifier7}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Classifier7 :}
         \PY{l+s+sd}{    6 Convolutional layers using stride=1, no dilatation and padding to assure same convolution, all having :}
         \PY{l+s+sd}{        \PYZhy{} kernel of size 3 (first 3 layers) or 5 (last 4 layers)}
         \PY{l+s+sd}{        \PYZhy{} double the number of feature maps received from the previous layer}
         \PY{l+s+sd}{        \PYZhy{} followed by ReLU non\PYZhy{}linearity }
         \PY{l+s+sd}{        \PYZhy{} and non\PYZhy{}overlapping max pooling with kernel of size 2 }
         \PY{l+s+sd}{        \PYZhy{} which means that each layer (made of those 3 steps) :}
         \PY{l+s+sd}{            \PYZhy{} receive as input n  feature maps of size 2m x 2m}
         \PY{l+s+sd}{            \PYZhy{} return  as outpu 2n feature maps of size  m x  m}
         \PY{l+s+sd}{    With the exeption of :}
         \PY{l+s+sd}{        \PYZhy{} the 4th and 6th layer does not have a max pooling}
         \PY{l+s+sd}{    After the convolutional part of the model, the original 3x64x64 input picture is now a 512x4x4 vector.}
         \PY{l+s+sd}{    The 7 conv. layers are followed by two fully connected layer :}
         \PY{l+s+sd}{        \PYZhy{} the first as ReLU activation}
         \PY{l+s+sd}{        \PYZhy{} the last is linear}
         \PY{l+s+sd}{    For the output of this model to be seen as a probabilie dist., it has to be fed to a F.softmax(...,dim=\PYZhy{}1)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self} \PY{p}{)}\PY{p}{:}
                 
                 \PY{n}{kernel\PYZus{}sz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
                 \PY{n}{pad} \PY{o}{=} \PY{n}{kernel\PYZus{}sz} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2} 
                 
                 \PY{n+nb}{super}\PY{p}{(}\PY{n}{Classifier7}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
                     \PY{c+c1}{\PYZsh{} Layer, input size = 64\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 2, input size = 32\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 3, input size = 16\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} Layer 4, input size = 8\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} Layer 5, input size = 8\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     
                     \PY{c+c1}{\PYZsh{} Layer 6, input size = 4\PYZca{}2}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}     
                 \PY{p}{)}
                 \PY{c+c1}{\PYZsh{} }
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{512}\PY{p}{,} \PY{l+m+mi}{512}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{512}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct2}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 \PY{k}{return} \PY{n}{x}
             
             \PY{k}{def} \PY{n+nf}{to\PYZus{}string}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{n}{depth\PYZus{}to\PYZus{}string} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The depth of this model is fixed to 8}\PY{l+s+s2}{\PYZdq{}}
                 \PY{k}{return} \PY{n}{depth\PYZus{}to\PYZus{}string} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}doc\PYZus{}\PYZus{}}
\end{Verbatim}

    Model with vgg-like architecture, inspired by :\\
https://pytorch.org/docs/0.4.0/\_modules/torchvision/models/vgg.html

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{class} \PY{n+nc}{VGGClassifier}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    VGGClassifier : a vgg\PYZhy{}like model :}
         \PY{l+s+sd}{    The first part of the model is a made of 2 types of layers:}
         \PY{l+s+sd}{        A \PYZhy{} a same convolution with kernel of size 3, padding of 1, no dilatation, stride = 1, with ReLU activations}
         \PY{l+s+sd}{        B \PYZhy{} non\PYZhy{}overlapping max pooling with kernel of size 2}
         \PY{l+s+sd}{    Each layer of type A :}
         \PY{l+s+sd}{        \PYZhy{} can change the number of feature channels i.e. takes n1 feature channels and returns n2}
         \PY{l+s+sd}{        \PYZhy{} will keep unchanged the size of the feature maps}
         \PY{l+s+sd}{    Each layer of type B :}
         \PY{l+s+sd}{        \PYZhy{} will keep unchanged the number of feature channels and divide by 2 the size of the feature maps}
         \PY{l+s+sd}{    The model takes as input a list channels\PYZus{}list that indicates which layers are of type A and B :}
         \PY{l+s+sd}{        \PYZhy{} the number indicates a layer of type A and correspond to the number of feature channels of its output}
         \PY{l+s+sd}{        \PYZhy{} \PYZbs{}\PYZsq{}M\PYZbs{}\PYZsq{} for max\PYZhy{}pooling indicates a layer of type B }
         \PY{l+s+sd}{    After the convolutional part of the model, the original 3x64x64 input picture is now a vector.}
         \PY{l+s+sd}{    If there is 6 \PYZbs{}\PYZsq{}M\PYZbs{}\PYZsq{} on the channels\PYZus{}list (because of the size of the input, there cannot be more than 6), }
         \PY{l+s+sd}{    the size of this vector is the number of feature maps of the last layer of the convolutional part.}
         \PY{l+s+sd}{    The convolutional part is followed by 3 fully connected layer, the first two have ReLU activations. The}
         \PY{l+s+sd}{    parameter size can be used to increase the size of this part of the model.}
         \PY{l+s+sd}{    For the output of this model to be seen as a probabilie dist., it has to be fed to a F.softmax(...,dim=\PYZhy{}1)}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}
                          \PY{n}{channels\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{150}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{250}\PY{p}{,}\PY{l+m+mi}{300}\PY{p}{,}\PY{l+m+mi}{350}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{400}\PY{p}{,}\PY{l+m+mi}{450}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{500}\PY{p}{,}\PY{l+m+mi}{525}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{550}\PY{p}{]}\PY{p}{,} 
                          \PY{n}{size} \PY{o}{=} \PY{l+m+mi}{500}
                         \PY{p}{)}\PY{p}{:}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size}   \PY{o}{=} \PY{n}{size}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{channels\PYZus{}list} \PY{o}{=} \PY{n}{channels\PYZus{}list}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{depth}  \PY{o}{=} \PY{l+m+mi}{0}
                 
                 \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{channels\PYZus{}list}\PY{p}{:}
                     \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:}
                         \PY{k}{continue}
                     \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{depth} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                     
                 \PY{n}{conv\PYZus{}out\PYZus{}channels} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{reversed}\PY{p}{(}\PY{n}{channels\PYZus{}list}\PY{p}{)} \PY{p}{:}
                     \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} 
                         \PY{k}{continue}
                     \PY{n}{conv\PYZus{}out\PYZus{}channels} \PY{o}{=} \PY{n}{i}
                     \PY{k}{break}
                     
                 \PY{n+nb}{super}\PY{p}{(}\PY{n}{VGGClassifier}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{features} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{make\PYZus{}layers}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{channels\PYZus{}list}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classifier} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{conv\PYZus{}out\PYZus{}channels}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                 \PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{features}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classifier}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                 \PY{k}{return} \PY{n}{x}
         
             \PY{k}{def} \PY{n+nf}{make\PYZus{}layers}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{channels\PYZus{}list}\PY{p}{)}\PY{p}{:}
                 \PY{n}{layers} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n}{in\PYZus{}channels} \PY{o}{=} \PY{l+m+mi}{3}
                 \PY{k}{for} \PY{n}{v} \PY{o+ow}{in} \PY{n}{channels\PYZus{}list}\PY{p}{:}
                     \PY{k}{if} \PY{n}{v} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                         \PY{n}{layers} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}
                     \PY{k}{else}\PY{p}{:}
                         \PY{n}{layers} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
                         \PY{n}{layers} \PY{o}{+}\PY{o}{=} \PY{p}{[}\PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{]}
                         \PY{n}{in\PYZus{}channels} \PY{o}{=} \PY{n}{v}
                 \PY{k}{return} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{o}{*}\PY{n}{layers}\PY{p}{)}
             
             \PY{k}{def} \PY{n+nf}{to\PYZus{}string}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{n}{depth\PYZus{}to\PYZus{}string} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The depth of this instance is : }\PY{l+s+si}{\PYZob{}d\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{d}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{depth}\PY{p}{)}
                 \PY{k}{return} \PY{n}{depth\PYZus{}to\PYZus{}string} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}doc\PYZus{}\PYZus{}}
\end{Verbatim}

    \section{Assignment Questions}\label{assignment-questions}

In the following, we specifically addressed the questions asked
regarding Problem 3:\\
\#\# Question 1

Describe the architecture (number of layers, filter sizes, pooling,
etc.), and report the number of parameters. You can take inspiration
from some modern deep neural network architectures such as the VGG
networks to improve the performance.

    \subsection{Number of parameters in each
model:}\label{number-of-parameters-in-each-model}

Using the following function, we may analyze the number of parameters in
each models:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{def} \PY{n+nf}{number\PYZus{}of\PYZus{}params}\PY{p}{(} \PY{n}{net} \PY{p}{,} \PY{n}{display\PYZus{}comp} \PY{o}{=} \PY{k+kc}{False} \PY{p}{)} \PY{p}{:}
             \PY{n}{nb\PYZus{}param}  \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{depth}     \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} count the number of different bias}
             \PY{n}{param\PYZus{}lst} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}
             \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{key}\PY{p}{,} \PY{n}{value}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(} \PY{n}{net}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)} \PY{p}{)} \PY{p}{:}
                 \PY{k}{if} \PY{n}{key}\PY{o}{.}\PY{n}{endswith}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{bias}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{p}{:}
                     \PY{n}{depth} \PY{o}{=} \PY{n}{depth} \PY{o}{+} \PY{l+m+mi}{1}
                     
                 \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{0} \PY{p}{:}
                     \PY{n}{param\PYZus{}lst} \PY{o}{=} \PY{n}{param\PYZus{}lst} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ (}\PY{l+s+si}{\PYZob{}:\PYZlt{}20\PYZcb{}}\PY{l+s+s2}{    }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(} \PY{n}{key} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}} \PY{p}{)}
                 \PY{k}{else} \PY{p}{:}
                     \PY{n}{param\PYZus{}lst} \PY{o}{=} \PY{n}{param\PYZus{}lst} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ (}\PY{l+s+si}{\PYZob{}:\PYZlt{}20\PYZcb{}}\PY{l+s+s2}{  + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(} \PY{n}{key} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}} \PY{p}{)}
                     
                     
                 \PY{n}{nb\PYZus{}param\PYZus{}tmp} \PY{o}{=} \PY{l+m+mi}{1}
                 
                 \PY{k}{for} \PY{n}{j} \PY{p}{,} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{value}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{p}{:}
                     \PY{k}{if} \PY{n}{j} \PY{o}{==} \PY{l+m+mi}{0} \PY{p}{:}
                         \PY{n}{param\PYZus{}lst} \PY{o}{=} \PY{n}{param\PYZus{}lst} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}xx\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(} \PY{n}{xx} \PY{o}{=} \PY{n}{x} \PY{p}{)} 
                     \PY{k}{else} \PY{p}{:}
                         \PY{n}{param\PYZus{}lst} \PY{o}{=} \PY{n}{param\PYZus{}lst} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+si}{\PYZob{}xx\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(} \PY{n}{xx} \PY{o}{=} \PY{n}{x} \PY{p}{)} 
                                        
                     \PY{n}{nb\PYZus{}param\PYZus{}tmp} \PY{o}{=} \PY{n}{nb\PYZus{}param\PYZus{}tmp} \PY{o}{*} \PY{n}{x}
                            
                 \PY{n}{nb\PYZus{}param} \PY{o}{=} \PY{n}{nb\PYZus{}param} \PY{o}{+} \PY{n}{nb\PYZus{}param\PYZus{}tmp}
                 
             \PY{k}{if} \PY{n}{display\PYZus{}comp}\PY{p}{:} 
                 \PY{n+nb}{print}\PY{p}{(} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{number of params = }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{nb\PYZus{}param} \PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{param\PYZus{}lst}  \PY{p}{)}
                 
             \PY{k}{return} \PY{n}{nb\PYZus{}param}\PY{p}{,} \PY{n}{depth}
           
\end{Verbatim}

    \subsubsection{Display a description of the architecture of each
models}\label{display-a-description-of-the-architecture-of-each-models}

Including the number of layers, kernel sizes, pooling, and a report of
the number of parameters.\\
The size of these models are around 2,6 and 13 millions parameters.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{list\PYZus{}of\PYZus{}models} \PY{o}{=} \PY{p}{[} 
             \PY{n}{Classifier5}\PY{p}{(}\PY{p}{)}\PY{p}{,}
             \PY{n}{Classifier7}\PY{p}{(}\PY{p}{)}\PY{p}{,}
             \PY{n}{VGGClassifier}\PY{p}{(}\PY{p}{)}
         \PY{p}{]}
         \PY{k}{for} \PY{n}{net} \PY{o+ow}{in} \PY{n}{list\PYZus{}of\PYZus{}models}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{net}\PY{o}{.}\PY{n}{to\PYZus{}string}\PY{p}{(}\PY{p}{)} \PY{p}{)}
             \PY{n}{\PYZus{}} \PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{number\PYZus{}of\PYZus{}params}\PY{p}{(} \PY{n}{net} \PY{p}{,} \PY{n}{display\PYZus{}comp} \PY{o}{=} \PY{k+kc}{True} \PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

The depth of this model is fixed to 8
    Classifier5 :
    7 Convolutional layers using stride=1, no dilatation and padding to assure same convolution, all having :
        - kernel of size 3 (first 3 layers) or 5 (last 4 layers)
        - double the number of feature maps received from the previous layer
        - followed by ReLU non-linearity 
        - and non-overlapping max pooling with kernel of size 2 
        - which means that each layer (made of those 3 steps) :
            - receive as input n  feature maps of size 2m x 2m
            - return  as outpu 2n feature maps of size  m x  m
    With the exeption of :
        - the 4th layer does not have a max pooling
        - the last layer does not have ReLU non-linearity
    After the convolutional part of the model, the original 3x64x64 input picture is now a 512x1x1 vector.
    The 7 conv. layers are followed by one fully connected linear layer
    For the output of this model to be seen as a probabilie dist., it has to be fed to a F.softmax({\ldots},dim=-1)
    
number of params =  2205602  =   
 (conv.0.weight)          16*3*5*5
 (conv.0.bias)          + 16
 (conv.3.weight)        + 32*16*5*5
 (conv.3.bias)          + 32
 (conv.6.weight)        + 64*32*5*5
 (conv.6.bias)          + 64
 (conv.9.weight)        + 128*64*3*3
 (conv.9.bias)          + 128
 (conv.11.weight)       + 256*128*3*3
 (conv.11.bias)         + 256
 (conv.14.weight)       + 256*256*3*3
 (conv.14.bias)         + 256
 (conv.17.weight)       + 512*256*3*3
 (conv.17.bias)         + 512
 (fct1b.weight)         + 2*512
 (fct1b.bias)           + 2

The depth of this model is fixed to 8
    Classifier7 :
    6 Convolutional layers using stride=1, no dilatation and padding to assure same convolution, all having :
        - kernel of size 3 (first 3 layers) or 5 (last 4 layers)
        - double the number of feature maps received from the previous layer
        - followed by ReLU non-linearity 
        - and non-overlapping max pooling with kernel of size 2 
        - which means that each layer (made of those 3 steps) :
            - receive as input n  feature maps of size 2m x 2m
            - return  as outpu 2n feature maps of size  m x  m
    With the exeption of :
        - the 4th and 6th layer does not have a max pooling
    After the convolutional part of the model, the original 3x64x64 input picture is now a 512x4x4 vector.
    The 7 conv. layers are followed by two fully connected layer :
        - the first as ReLU activation
        - the last is linear
    For the output of this model to be seen as a probabilie dist., it has to be fed to a F.softmax({\ldots},dim=-1)
    
number of params =  5810338  =   
 (conv.0.weight)          16*3*5*5
 (conv.0.bias)          + 16
 (conv.3.weight)        + 32*16*5*5
 (conv.3.bias)          + 32
 (conv.6.weight)        + 64*32*5*5
 (conv.6.bias)          + 64
 (conv.9.weight)        + 128*64*3*3
 (conv.9.bias)          + 128
 (conv.11.weight)       + 256*128*3*3
 (conv.11.bias)         + 256
 (conv.14.weight)       + 512*256*3*3
 (conv.14.bias)         + 512
 (fct1.weight)          + 512*8192
 (fct1.bias)            + 512
 (fct2.weight)          + 2*512
 (fct2.bias)            + 2

The depth of this instance is : 12
    VGGClassifier : a vgg-like model :
    The first part of the model is a made of 2 types of layers:
        A - a same convolution with kernel of size 3, padding of 1, no dilatation, stride = 1, with ReLU activations
        B - non-overlapping max pooling with kernel of size 2
    Each layer of type A :
        - can change the number of feature channels i.e. takes n1 feature channels and returns n2
        - will keep unchanged the size of the feature maps
    Each layer of type B :
        - will keep unchanged the number of feature channels and divide by 2 the size of the feature maps
    The model takes as input a list channels\_list that indicates which layers are of type A and B :
        - the number indicates a layer of type A and correspond to the number of feature channels of its output
        - 'M' for max-pooling indicates a layer of type B 
    After the convolutional part of the model, the original 3x64x64 input picture is now a vector.
    If there is 6 'M' on the channels\_list, the size of this vector is the number of feature maps of the last 
    layer of the convolutional part.
    The convolutional part is followed by 3 fully connected layer, the first two have ReLU activations. The
    parameter size can be used to increase the size of this part of the model.
    For the output of this model to be seen as a probabilie dist., it has to be fed to a F.softmax({\ldots},dim=-1)
    
number of params =  12918427  =   
 (features.0.weight)      50*3*3*3
 (features.0.bias)      + 50
 (features.3.weight)    + 100*50*3*3
 (features.3.bias)      + 100
 (features.6.weight)    + 150*100*3*3
 (features.6.bias)      + 150
 (features.8.weight)    + 200*150*3*3
 (features.8.bias)      + 200
 (features.11.weight)   + 250*200*3*3
 (features.11.bias)     + 250
 (features.13.weight)   + 300*250*3*3
 (features.13.bias)     + 300
 (features.15.weight)   + 350*300*3*3
 (features.15.bias)     + 350
 (features.18.weight)   + 400*350*3*3
 (features.18.bias)     + 400
 (features.20.weight)   + 450*400*3*3
 (features.20.bias)     + 450
 (features.23.weight)   + 500*450*3*3
 (features.23.bias)     + 500
 (features.25.weight)   + 525*500*3*3
 (features.25.bias)     + 525
 (features.28.weight)   + 550*525*3*3
 (features.28.bias)     + 550
 (classifier.0.weight)  + 500*550
 (classifier.0.bias)    + 500
 (classifier.2.weight)  + 500*500
 (classifier.2.bias)    + 500
 (classifier.4.weight)  + 2*500
 (classifier.4.bias)    + 2

    \end{Verbatim}

    \paragraph{Test a model}\label{test-a-model}

To see if it works and if its output has the required shape

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{mynet} \PY{o}{=} \PY{n}{VGGClassifier}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} mynet = torchvision.models.vgg16( pretrained=False, num\PYZus{}classes=2, init\PYZus{}weights=True )}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{mynet}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{16}
         \PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
         \PY{n}{criterion} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}
         \PY{n}{want\PYZus{}to\PYZus{}test} \PY{o}{=} \PY{k+kc}{True}
         \PY{k}{if} \PY{n}{want\PYZus{}to\PYZus{}test}\PY{p}{:}
             \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)} \PY{p}{:}
                 \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{data} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} get the inputs}
                     \PY{n}{inputs}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{data}
                     \PY{n}{inputs}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{inputs}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{,} \PY{n}{labels}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                     \PY{n}{outputs} \PY{o}{=} \PY{n}{mynet}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}
                     \PY{n}{loss} \PY{o}{=} \PY{n}{criterion}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(} \PY{n}{outputs}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)} \PY{p}{,} \PY{n}{labels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)} \PY{p}{)}
                     \PY{n+nb}{print}\PY{p}{(} \PY{n}{loss} \PY{p}{)}
                     \PY{k}{break}
         \PY{k}{del} \PY{n}{mynet}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([16, 2]) torch.Size([16])
tensor(0.7038, device='cuda:0')

    \end{Verbatim}

    \section{Training}\label{training}

training algorithm below

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{} make sound once done, should only be used to wrap a function that returns nothing }
         \PY{k}{def} \PY{n+nf}{make\PYZus{}sound}\PY{p}{(}\PY{n}{func}\PY{p}{)}\PY{p}{:}
             \PY{k}{def} \PY{n+nf}{wrapper\PYZus{}make\PYZus{}sound}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}\PY{p}{:}
                 \PY{n}{func}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
                 \PY{n}{wave} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{l+m+mf}{1.5}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{o}{*}\PY{l+m+mi}{400}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{10000}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{10000}\PY{p}{)} 
                 \PY{n}{audio} \PY{o}{=} \PY{n}{Audio}\PY{p}{(}\PY{n}{wave}\PY{p}{,} \PY{n}{rate}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,} \PY{n}{autoplay}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                 \PY{k}{if} \PY{n}{want\PYZus{}lound\PYZus{}warning} \PY{p}{:}
                     \PY{k}{return} \PY{n}{audio}
             \PY{k}{return} \PY{n}{wrapper\PYZus{}make\PYZus{}sound}
         
         \PY{c+c1}{\PYZsh{} measure time with cuda events}
         \PY{k}{def} \PY{n+nf}{display\PYZus{}timer}\PY{p}{(}\PY{n}{func}\PY{p}{)}\PY{p}{:}
             \PY{k}{def} \PY{n+nf}{wrapper\PYZus{}display\PYZus{}timer}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}\PY{p}{:}
                 \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
                 \PY{n}{start} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{Event}\PY{p}{(}\PY{n}{enable\PYZus{}timing}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                 \PY{n}{end}   \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{Event}\PY{p}{(}\PY{n}{enable\PYZus{}timing}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                 \PY{n}{start}\PY{o}{.}\PY{n}{record}\PY{p}{(}\PY{p}{)}
                 \PY{n}{res} \PY{o}{=} \PY{n}{func}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
                 \PY{n}{end}\PY{o}{.}\PY{n}{record}\PY{p}{(}\PY{p}{)}
                 \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time required = }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{start}\PY{o}{.}\PY{n}{elapsed\PYZus{}time}\PY{p}{(}\PY{n}{end}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.001} \PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ s }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                 \PY{k}{return} \PY{n}{res}
             \PY{k}{return} \PY{n}{wrapper\PYZus{}display\PYZus{}timer}
         
         \PY{n+nd}{@make\PYZus{}sound}
         \PY{n+nd}{@display\PYZus{}timer}
         \PY{k}{def} \PY{n+nf}{training\PYZus{}phase}\PY{p}{(} \PY{n}{net}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{regul}\PY{p}{,} \PY{n}{patience}\PY{p}{,} \PY{n}{avg\PYZus{}loss}\PY{p}{,} \PY{n}{accuracy}\PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{,} \PY{n}{valid\PYZus{}loader}\PY{p}{,} \PY{n}{state\PYZus{}dict\PYZus{}list} \PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} regul    : regularization parameter}
             \PY{c+c1}{\PYZsh{} patience : number of epoch without improvement before halting the training}
             
             \PY{n}{criterion\PYZus{}sum} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} to sum the loss of samples in a mini\PYZhy{}batch}
             \PY{n}{criterion}     \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)} 
             \PY{n}{max\PYZus{}valid\PYZus{}acc}      \PY{o}{=} \PY{l+m+mi}{50}
             \PY{n}{waiting\PYZus{}period}     \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{abandon\PYZus{}train}      \PY{o}{=} \PY{k+kc}{False}
             \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(} \PY{n}{nb\PYZus{}epoch} \PY{p}{)}\PY{p}{:}  \PY{c+c1}{\PYZsh{} loop over the dataset multiple times}
         
                 \PY{n}{running\PYZus{}loss}  \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)} 
                 \PY{n}{correct}       \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)}
                 \PY{n}{total}         \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)} 
                 \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{data} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} get the inputs}
                     \PY{n}{inputs}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{data}
                     \PY{n}{inputs}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{inputs}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{,} \PY{n}{labels}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} zero the parameter gradients}
                     \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} forward + backward + optimize}
                     \PY{n}{outputs} \PY{o}{=} \PY{n}{net}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
                     
                     \PY{c+c1}{\PYZsh{} we compute the L\PYZus{}2 norm of the weigths, skipping the biases}
                     \PY{n}{norm\PYZus{}L2} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{dtype} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
                     \PY{k}{for} \PY{n}{param} \PY{o+ow}{in} \PY{n}{net}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)} \PY{p}{:}
                         \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{param}\PY{o}{.}\PY{n}{shape}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1} \PY{p}{:} \PY{c+c1}{\PYZsh{} skip biases}
                             \PY{k}{continue}
                         \PY{n}{norm\PYZus{}L2} \PY{o}{+}\PY{o}{=} \PY{n}{param}\PY{o}{.}\PY{n}{pow}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
                     \PY{n}{norm\PYZus{}L2} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{norm\PYZus{}L2}\PY{p}{)}
                     
                     \PY{n}{loss} \PY{o}{=} \PY{n}{criterion}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{labels}\PY{p}{)} \PY{o}{+} \PY{n}{regul}\PY{o}{*}\PY{n}{norm\PYZus{}L2}
                     \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
                     \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} compute the correctness of the output labels}
                     \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)} \PY{p}{:}
                         \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                         \PY{n}{total}   \PY{o}{+}\PY{o}{=} \PY{n}{labels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
                         \PY{n}{correct} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{predicted} \PY{o}{==} \PY{n}{labels}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
                         \PY{n}{loss\PYZus{}sum} \PY{o}{=} \PY{n}{criterion\PYZus{}sum}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} print statistics}
                     \PY{n}{running\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss\PYZus{}sum}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
                 \PY{k}{else} \PY{p}{:} \PY{c+c1}{\PYZsh{} print every epoch}
                     \PY{n}{avg\PYZus{}loss}\PY{p}{[}\PY{n}{epoch}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{running\PYZus{}loss} \PY{o}{/} \PY{n}{total}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}
                     \PY{n}{accuracy}\PY{p}{[}\PY{n}{epoch}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{100} \PY{o}{*} \PY{n}{correct}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)} \PY{o}{/} \PY{n}{total}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}
                     
                     \PY{n}{valid\PYZus{}acc}\PY{p}{,} \PY{n}{valid\PYZus{}loss} \PY{o}{=} \PY{n}{measure\PYZus{}single\PYZus{}accuracy\PYZus{}and\PYZus{}loss}\PY{p}{(}\PY{n}{net}\PY{p}{,} \PY{n}{valid\PYZus{}loader}\PY{p}{,} \PY{n}{criterion\PYZus{}sum} \PY{p}{)}
                     \PY{n}{avg\PYZus{}loss}\PY{p}{[}\PY{n}{epoch}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{valid\PYZus{}loss}
                     \PY{n}{accuracy}\PY{p}{[}\PY{n}{epoch}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{valid\PYZus{}acc}
                     \PY{k}{if} \PY{n}{valid\PYZus{}acc} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}valid\PYZus{}acc}\PY{p}{:} \PY{c+c1}{\PYZsh{} found new best accuracy}
                         \PY{n}{max\PYZus{}valid\PYZus{}acc} \PY{o}{=} \PY{n}{valid\PYZus{}acc}
                         \PY{n}{waiting\PYZus{}period}\PY{o}{=} \PY{l+m+mi}{0}
                     \PY{k}{else} \PY{p}{:}
                         \PY{n}{waiting\PYZus{}period}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
                 
                     \PY{n+nb}{print}\PY{p}{(} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epoch = }\PY{l+s+si}{\PYZpc{}3d}\PY{l+s+s1}{, train loss = }\PY{l+s+si}{\PYZpc{}.6f}\PY{l+s+s1}{ , train accuracy = }\PY{l+s+si}{\PYZpc{}3f}\PY{l+s+s1}{ , valid loss = }\PY{l+s+si}{\PYZpc{}.6f}\PY{l+s+s1}{ , valid accuracy = }\PY{l+s+si}{\PYZpc{}3f}\PY{l+s+s1}{\PYZsq{}} 
                                   \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{epoch} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{avg\PYZus{}loss}\PY{p}{[}\PY{n}{epoch}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{accuracy}\PY{p}{[}\PY{n}{epoch}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{avg\PYZus{}loss}\PY{p}{[}\PY{n}{epoch}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{accuracy}\PY{p}{[}\PY{n}{epoch}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{p}{)}
                          \PY{p}{)} 
                     \PY{c+c1}{\PYZsh{} save the current model\PYZsq{}s state\PYZus{}dictionnary}
                     \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
                     \PY{n}{tmp\PYZus{}state\PYZus{}dict} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
                     \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{v} \PY{o+ow}{in} \PY{n}{net}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                         \PY{n}{tmp\PYZus{}state\PYZus{}dict}\PY{p}{[}\PY{n}{k}\PY{p}{]} \PY{o}{=} \PY{n}{v}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
                     \PY{n}{state\PYZus{}dict\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(} \PY{n}{tmp\PYZus{}state\PYZus{}dict} \PY{p}{)}
                     \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}   
                     
                     \PY{k}{if} \PY{n}{waiting\PYZus{}period} \PY{o}{\PYZgt{}} \PY{n}{patience} \PY{p}{:} \PY{c+c1}{\PYZsh{} too much time since the last improvement}
                         \PY{n}{abandon\PYZus{}train} \PY{o}{=} \PY{k+kc}{True}
                 \PY{k}{if} \PY{n}{abandon\PYZus{}train} \PY{p}{:}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Early stopping}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                     \PY{k}{break}
             \PY{k}{else} \PY{p}{:} 
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Finished Training}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} measure accuracy of a single net, returns the accuracy}
         \PY{k}{def} \PY{n+nf}{measure\PYZus{}single\PYZus{}accuracy\PYZus{}and\PYZus{}loss}\PY{p}{(} \PY{n}{net}\PY{p}{,} \PY{n}{loader}\PY{p}{,} \PY{n}{criterion} \PY{p}{)}\PY{p}{:}
             \PY{n}{accuracy} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
             \PY{n}{avg\PYZus{}loss} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{]}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
             \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                 \PY{n}{correct} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
                 \PY{n}{total}   \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
                 \PY{k}{for} \PY{n}{data} \PY{o+ow}{in} \PY{n}{loader}\PY{p}{:}
                     \PY{n}{images}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{data}
                     \PY{n}{images}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{images}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{,} \PY{n}{labels}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                     \PY{n}{outputs} \PY{o}{=} \PY{n}{net}\PY{p}{(}\PY{n}{images}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
                     \PY{n}{loss} \PY{o}{=} \PY{n}{criterion}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
                     \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                     \PY{n}{total}   \PY{o}{+}\PY{o}{=} \PY{n}{labels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
                     \PY{n}{correct} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{predicted} \PY{o}{==} \PY{n}{labels}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
                     \PY{n}{avg\PYZus{}loss}\PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
                 \PY{n}{accuracy} \PY{o}{=} \PY{l+m+mi}{100} \PY{o}{*} \PY{n}{correct}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)} \PY{o}{/} \PY{n}{total}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}
                 \PY{n}{avg\PYZus{}loss} \PY{o}{=} \PY{n}{avg\PYZus{}loss}\PY{o}{/}\PY{n}{total}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}
             \PY{k}{return} \PY{n}{accuracy}\PY{p}{,} \PY{n}{avg\PYZus{}loss}      
\end{Verbatim}

    \subsubsection{Plotting}\label{plotting}

Plotting function use to display accuracy and loss of a model across
epochs during its training.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{} display 2 plots, accuracy and loss across epoch, their .shape must be n x 2, }
         \PY{c+c1}{\PYZsh{} want\PYZus{}log indicates that user wants to save the plot to a file}
         \PY{c+c1}{\PYZsh{} filename should not contains the extension of the file}
         \PY{k}{def} \PY{n+nf}{plot\PYZus{}1d\PYZus{}acc\PYZus{}and\PYZus{}loss}\PY{p}{(}\PY{n}{net}\PY{p}{,} \PY{n}{accuracy}\PY{p}{,} \PY{n}{loss}\PY{p}{,} \PY{n}{path\PYZus{}to\PYZus{}save}\PY{p}{,} \PY{n}{filename}\PY{p}{,} 
                                  \PY{n}{net\PYZus{}name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{want\PYZus{}log} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{n}{font\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{16} \PY{p}{)}\PY{p}{:}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{font.size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{font\PYZus{}size}\PY{p}{\PYZcb{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{figure.figsize}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{figsize} 
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{n}{left}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{bottom}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{right}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{top}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{wspace}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{hspace}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}
             
             \PY{n}{nb\PYZus{}epoch} \PY{o}{=}  \PY{n}{accuracy}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             
             \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{p}{)}
             
             \PY{n}{y1a} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{accuracy}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
             \PY{n}{y1b} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{accuracy}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
             \PY{n}{line1a\PYZus{}label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy on the validation set}\PY{l+s+s2}{\PYZdq{}}
             \PY{n}{line1b\PYZus{}label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy on the training   set}\PY{l+s+s2}{\PYZdq{}}
             
             \PY{n}{y2a} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{loss}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
             \PY{n}{y2b} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{loss}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
             \PY{n}{line2a\PYZus{}label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{avg loss on the validation set}\PY{l+s+s2}{\PYZdq{}}
             \PY{n}{line2b\PYZus{}label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{avg loss on the training   set}\PY{l+s+s2}{\PYZdq{}}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         
             \PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{75}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{black}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
             \PY{n}{line1a}\PY{p}{,} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y1a}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{line1a\PYZus{}label}\PY{p}{)}
             \PY{n}{line1a}\PY{o}{.}\PY{n}{set\PYZus{}dashes}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 2pt line, 2pt break}
         
             \PY{n}{line1b}\PY{p}{,} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y1b}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{line1b\PYZus{}label}\PY{p}{)}
             \PY{n}{line1b}\PY{o}{.}\PY{n}{set\PYZus{}dashes}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 2pt line, 2pt break}
         
             \PY{n}{str\PYZus{}title1} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy during the training}\PY{l+s+s2}{\PYZdq{}}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{str\PYZus{}title1}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
             
             \PY{n}{line2a}\PY{p}{,} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y2a}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{line2a\PYZus{}label}\PY{p}{)}
             \PY{n}{line2a}\PY{o}{.}\PY{n}{set\PYZus{}dashes}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 2pt line, 2pt break}
         
             \PY{n}{line2b}\PY{p}{,} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y2b}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{n}{line2b\PYZus{}label}\PY{p}{)}
             \PY{n}{line2b}\PY{o}{.}\PY{n}{set\PYZus{}dashes}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}  \PY{c+c1}{\PYZsh{} 2pt line, 2pt break}
             
             \PY{n}{str\PYZus{}title1} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss during the training}\PY{l+s+s2}{\PYZdq{}}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{str\PYZus{}title1}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} ytop = ...}
             \PY{c+c1}{\PYZsh{} plt.ylim(0, ytop)     \PYZsh{} set the ylim to bottom, top}
             
             \PY{k}{if} \PY{n}{net\PYZus{}name} \PY{o}{!=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}} \PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{n}{net\PYZus{}name}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{n}{font\PYZus{}size}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} path\PYZus{}to\PYZus{}save = \PYZdq{}./output/\PYZdq{} }
             \PY{c+c1}{\PYZsh{} filename     = datetime.datetime.now().strftime(\PYZdq{}\PYZpc{}Y\PYZpc{}B\PYZpc{}d\PYZus{}\PYZpc{}p\PYZpc{}IH\PYZpc{}MM\PYZdq{})}
             \PY{k}{if} \PY{n}{want\PYZus{}log} \PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}save} \PY{o}{+} \PY{n}{filename} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.png}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \subsubsection{Another ploting function}\label{another-ploting-function}

This function takes an Nx1-array of number "accuracy", a Nx2-array
"hyper\_param".\\
At position hyper\_param{[}i,:{]} it shows a point which area is an
increasing function of accuracy{[}i{]}.\\
The scaling depends on the content of accuracy and on the scaling
parameters.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{k}{def} \PY{n+nf}{plot\PYZus{}accuracy\PYZus{}2d}\PY{p}{(}\PY{n}{accuracy}\PY{p}{,}\PY{n}{hyper\PYZus{}param}\PY{p}{,}\PY{n}{path\PYZus{}to\PYZus{}save}\PY{p}{,}\PY{n}{filename}\PY{p}{,}\PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{axis\PYZus{}label}\PY{o}{=}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,} \PY{n}{want\PYZus{}log}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}\PY{n}{scaling}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{font\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{16}   
             \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{font.size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{font\PYZus{}size}\PY{p}{\PYZcb{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{figure.figsize}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{figsize}
             \PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{n}{left}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{bottom}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{right}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{top}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{wspace}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{hspace}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}
             
             \PY{n}{x} \PY{o}{=} \PY{n}{hyper\PYZus{}param}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{y} \PY{o}{=} \PY{n}{hyper\PYZus{}param}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
         
             \PY{n}{N} \PY{o}{=} \PY{n}{hyper\PYZus{}param\PYZus{}sequence}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
             \PY{n}{val} \PY{o}{=} \PY{n}{accuracy}
             \PY{n}{val} \PY{o}{=} \PY{n}{val} \PY{o}{\PYZhy{}} \PY{n}{val}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
             \PY{n}{val} \PY{o}{=} \PY{n}{val} \PY{o}{/} \PY{n}{val}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} 
         
             \PY{c+c1}{\PYZsh{} colors = np.ones(N)*(0.2)}
             \PY{n}{area} \PY{o}{=} \PY{n}{scaling}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{val}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{n}{scaling}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         
             \PY{n}{str\PYZus{}title1} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy of CNN, trained using different hyper\PYZhy{}parameters }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}
             \PY{n}{str\PYZus{}title2} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy range from }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{min:.}\PY{l+s+si}{\PYZob{}prec\PYZcb{}}\PY{l+s+s2}{f\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{(area of }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{rmin:.}\PY{l+s+si}{\PYZob{}prec\PYZcb{}}\PY{l+s+s2}{f\PYZcb{}) to }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{max:.}\PY{l+s+si}{\PYZob{}prec\PYZcb{}}\PY{l+s+s2}{f\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{(area of }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{rmax:.}\PY{l+s+si}{\PYZob{}prec\PYZcb{}}\PY{l+s+s2}{f\PYZcb{}) }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                   \PY{n+nb}{min}  \PY{o}{=} \PY{n}{\PYZus{}val}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                   \PY{n}{rmin} \PY{o}{=} \PY{n}{area}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                   \PY{n+nb}{max}  \PY{o}{=} \PY{n}{\PYZus{}val}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                   \PY{n}{rmax} \PY{o}{=} \PY{n}{area}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                   \PY{n}{prec} \PY{o}{=} \PY{l+m+mi}{1}
                 \PY{p}{)}
             
             \PY{n}{str\PYZus{}title} \PY{o}{=} \PY{n}{title}
             \PY{k}{if} \PY{n}{str\PYZus{}title} \PY{p}{:}
                 \PY{n}{str\PYZus{}title} \PY{o}{=} \PY{n}{str\PYZus{}title} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}
             \PY{n}{str\PYZus{}title} \PY{o}{=} \PY{n}{str\PYZus{}title} \PY{o}{+} \PY{n}{str\PYZus{}title1} \PY{o}{+} \PY{n}{str\PYZus{}title2}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{str\PYZus{}title}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{axis\PYZus{}label}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{n}{axis\PYZus{}label}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             
             \PY{n}{xbot} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
             \PY{n}{xtop} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}
             \PY{n}{xeps} \PY{o}{=} \PY{p}{(}\PY{n}{xtop}\PY{o}{\PYZhy{}}\PY{n}{xbot}\PY{p}{)}\PY{o}{/}\PY{l+m+mf}{100.0}
             \PY{n}{ybot} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
             \PY{n}{ytop} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}
             \PY{n}{yeps} \PY{o}{=} \PY{p}{(}\PY{n}{ytop}\PY{o}{\PYZhy{}}\PY{n}{ybot}\PY{p}{)}\PY{o}{/}\PY{l+m+mf}{100.0}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{n}{ybot}\PY{o}{\PYZhy{}}\PY{n}{yeps}\PY{p}{,} \PY{n}{ytop}\PY{o}{+}\PY{n}{yeps}\PY{p}{)}     \PY{c+c1}{\PYZsh{} set the ylim to bottom, top}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{xbot}\PY{o}{\PYZhy{}}\PY{n}{xeps}\PY{p}{,} \PY{n}{xtop}\PY{o}{+}\PY{n}{xeps}\PY{p}{)}     \PY{c+c1}{\PYZsh{} set the xlim to bottom, top}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{n}{area}\PY{p}{)}
             \PY{k}{if} \PY{n}{want\PYZus{}log} \PY{p}{:}
                     \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}save} \PY{o}{+} \PY{n}{filename} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.png}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}   
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \subsubsection{Initialization method}\label{initialization-method}

We use glorot uniform initialization

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k}{def} \PY{n+nf}{glorot\PYZus{}init} \PY{p}{(} \PY{n}{layer} \PY{p}{)} \PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Weiths are generated from U[\PYZhy{}d,d] where d = sqrt(6/(fan\PYZus{}in + fan\PYZus{}out)), biases are set to zero}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{if} \PY{n+nb}{type}\PY{p}{(}\PY{n}{layer}\PY{p}{)} \PY{o}{==} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear} \PY{o+ow}{or} \PY{n+nb}{type}\PY{p}{(}\PY{n}{layer}\PY{p}{)} \PY{o}{==} \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d} \PY{p}{:}
                 \PY{n}{init}\PY{o}{.}\PY{n}{xavier\PYZus{}uniform\PYZus{}}\PY{p}{(} \PY{n}{layer}\PY{o}{.}\PY{n}{weight} \PY{p}{,} \PY{n}{gain}\PY{o}{=}\PY{l+m+mi}{1} \PY{p}{)}
                 \PY{n}{layer}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{)}
\end{Verbatim}

    \subsection{Question 2}\label{question-2}

Plot the training error and validation error curves, along with the
training and validation losses. Comment on them. What techniques (you
did not implement) could be useful to improve the validation
performance. How does your validation performance compare to the test
set performance (that you can only get in Kaggle).

    \subsubsection{The training}\label{the-training}

Later in the notebook, we explain how we have searched for good choice
for 3 hyper-parameters : learning rate, batch size and regularisation
parameter. These suggestion are listed as comments. They take into
account :\\
- if regularization is used\\
- which type of data augmentation is used - which model is used\\
Our best performing model achieves (with early stop and retrieving the
state of the model that has among the best accuracy and minimal average
loss on the validation dataset) :\\
- 86.59\% accuracy on the test set accuracy on the validation dataset\\
- 86.51\% accuracy on the kaggle test dataset

This model is a VGGClassifier with default arguments and this is the one
that is used for the rest of the notebook.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{n}{net1} \PY{o}{=} \PY{n}{VGGClassifier}\PY{p}{(}\PY{p}{)}
         \PY{n}{net1}\PY{o}{.}\PY{n}{apply}\PY{p}{(} \PY{n}{glorot\PYZus{}init} \PY{p}{)}  
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{net1}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{c+c1}{\PYZsh{} define the loss function as the cross entropy and choose a learning rate that works well :}
         \PY{c+c1}{\PYZsh{} what we have found that works best using hyper\PYZhy{}parameter space search }
         \PY{c+c1}{\PYZsh{} using Classifier5}
         \PY{c+c1}{\PYZsh{} using no regularization :}
         \PY{c+c1}{\PYZsh{} using low     data augmentation : lr = 0.0003392602539062500, batch size = 21}
         \PY{c+c1}{\PYZsh{} using medium  data augmentation : lr = 0.0005646362304687499, batch size = 18}
         \PY{c+c1}{\PYZsh{} using high    data augmentation : lr = 0.0020221459960937504, batch size = 54}
         \PY{c+c1}{\PYZsh{} using regularisation :}
         \PY{c+c1}{\PYZsh{} using low     data augmentation : lr = 0.00236848           , batch size = 20, regul = 0.00522644 }
         \PY{c+c1}{\PYZsh{} using med\PYZhy{}low data augmentation : lr = 0.00129663           , batch size = 20, regul = 0.00378247 }
         \PY{c+c1}{\PYZsh{} using Classifier7 and regularisation:}
         \PY{c+c1}{\PYZsh{} using medium data augmentation : lr = 0.00110307            , batch size = 20, regul = 0.02835189 }
         \PY{c+c1}{\PYZsh{} using VGGClassifier() and regularisation:}
         \PY{c+c1}{\PYZsh{} using med\PYZhy{}low data augmentation : lr = 0.008962506103515625 , batch size = 78, regul = 0.00128163}
         
         \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.008962506}
         \PY{n}{optimizer}      \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{net1}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,} \PY{n}{momentum}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{weight\PYZus{}decay}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{patience}       \PY{o}{=} \PY{l+m+mi}{10}
         \PY{n}{regularization} \PY{o}{=} \PY{l+m+mf}{0.00128163}
         \PY{n}{nb\PYZus{}epoch}  \PY{o}{=} \PY{l+m+mi}{50}
         
         \PY{n}{train\PYZus{}batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{78}
         \PY{n}{valid\PYZus{}batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{64}
         \PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}augm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{train\PYZus{}batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{valid\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{valid\PYZus{}batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{net1\PYZus{}state\PYZus{}dict\PYZus{}list} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}        \PY{c+c1}{\PYZsh{} we save (all) the intermediate state of the model during the learning phase}
         
         \PY{c+c1}{\PYZsh{} accuracy and average loss across epoch, 0 (resp. 1) correspond to the training (reps. validation)}
         \PY{n}{avg\PYZus{}loss1} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{nb\PYZus{}epoch}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)}
         \PY{n}{accuracy1} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{nb\PYZus{}epoch}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{n}{training\PYZus{}phase}\PY{p}{(} \PY{n}{net1}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{regularization}\PY{p}{,} \PY{n}{patience}\PY{p}{,} \PY{n}{avg\PYZus{}loss1}\PY{p}{,} \PY{n}{accuracy1}\PY{p}{,} 
                         \PY{n}{train\PYZus{}loader}\PY{p}{,} \PY{n}{valid\PYZus{}loader}\PY{p}{,} \PY{n}{net1\PYZus{}state\PYZus{}dict\PYZus{}list} \PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
epoch =   1, train loss = 0.685922 , train accuracy = 55.119728 , valid loss = 0.683861 , valid accuracy = 54.877438
epoch =   2, train loss = 0.672265 , train accuracy = 58.536587 , valid loss = 0.659204 , valid accuracy = 59.729866
epoch =   3, train loss = 0.655578 , train accuracy = 61.786766 , valid loss = 0.694026 , valid accuracy = 54.427212
epoch =   4, train loss = 0.640317 , train accuracy = 63.536861 , valid loss = 0.626098 , valid accuracy = 64.832413
epoch =   5, train loss = 0.622389 , train accuracy = 65.964775 , valid loss = 0.585430 , valid accuracy = 69.184593
epoch =   6, train loss = 0.600616 , train accuracy = 67.775986 , valid loss = 0.573021 , valid accuracy = 70.535271
epoch =   7, train loss = 0.588853 , train accuracy = 69.192734 , valid loss = 0.560543 , valid accuracy = 71.335670
epoch =   8, train loss = 0.568956 , train accuracy = 70.420578 , valid loss = 0.540174 , valid accuracy = 73.086540
epoch =   9, train loss = 0.548063 , train accuracy = 72.265129 , valid loss = 0.537523 , valid accuracy = 72.336166
epoch =  10, train loss = 0.531182 , train accuracy = 73.687424 , valid loss = 0.503379 , valid accuracy = 74.937469
epoch =  11, train loss = 0.504645 , train accuracy = 75.098618 , valid loss = 0.482370 , valid accuracy = 76.888443
epoch =  12, train loss = 0.491531 , train accuracy = 76.232010 , valid loss = 0.532841 , valid accuracy = 72.886444
epoch =  13, train loss = 0.478888 , train accuracy = 77.054283 , valid loss = 0.597730 , valid accuracy = 70.135071
epoch =  14, train loss = 0.462258 , train accuracy = 78.237679 , valid loss = 0.467779 , valid accuracy = 77.638817
epoch =  15, train loss = 0.451611 , train accuracy = 78.771042 , valid loss = 0.450362 , valid accuracy = 78.389198
epoch =  16, train loss = 0.439693 , train accuracy = 79.487747 , valid loss = 0.432755 , valid accuracy = 79.539772
epoch =  17, train loss = 0.419826 , train accuracy = 80.526695 , valid loss = 0.421892 , valid accuracy = 80.340172
epoch =  18, train loss = 0.407747 , train accuracy = 81.354523 , valid loss = 0.429784 , valid accuracy = 80.940468
epoch =  19, train loss = 0.396293 , train accuracy = 81.898994 , valid loss = 0.418983 , valid accuracy = 80.090042
epoch =  20, train loss = 0.382143 , train accuracy = 82.532364 , valid loss = 0.389223 , valid accuracy = 82.441223
epoch =  21, train loss = 0.375428 , train accuracy = 82.926826 , valid loss = 0.386734 , valid accuracy = 81.590797
epoch =  22, train loss = 0.358822 , train accuracy = 83.871323 , valid loss = 0.385910 , valid accuracy = 81.890945
epoch =  23, train loss = 0.352306 , train accuracy = 84.193565 , valid loss = 0.431182 , valid accuracy = 79.089546
epoch =  24, train loss = 0.336355 , train accuracy = 85.143616 , valid loss = 0.431225 , valid accuracy = 79.639816
epoch =  25, train loss = 0.323309 , train accuracy = 85.838104 , valid loss = 0.386133 , valid accuracy = 82.341171
epoch =  26, train loss = 0.314348 , train accuracy = 86.049225 , valid loss = 0.374538 , valid accuracy = 83.991997
epoch =  27, train loss = 0.298575 , train accuracy = 87.149284 , valid loss = 0.399724 , valid accuracy = 83.091545
epoch =  28, train loss = 0.288547 , train accuracy = 87.915993 , valid loss = 0.410586 , valid accuracy = 82.091049
epoch =  29, train loss = 0.283947 , train accuracy = 87.877106 , valid loss = 0.358216 , valid accuracy = 84.792397
epoch =  30, train loss = 0.266959 , train accuracy = 88.527138 , valid loss = 0.360780 , valid accuracy = 83.891945
epoch =  31, train loss = 0.259971 , train accuracy = 88.916054 , valid loss = 0.342412 , valid accuracy = 84.742371
epoch =  32, train loss = 0.246764 , train accuracy = 89.477196 , valid loss = 0.399721 , valid accuracy = 81.490746
epoch =  33, train loss = 0.241338 , train accuracy = 89.971664 , valid loss = 0.341208 , valid accuracy = 85.742874
epoch =  34, train loss = 0.228192 , train accuracy = 90.377243 , valid loss = 0.360942 , valid accuracy = 84.742371
epoch =  35, train loss = 0.220616 , train accuracy = 90.977280 , valid loss = 0.390295 , valid accuracy = 85.892944
epoch =  36, train loss = 0.211273 , train accuracy = 91.277290 , valid loss = 0.386609 , valid accuracy = 85.342674
epoch =  37, train loss = 0.202138 , train accuracy = 91.838432 , valid loss = 0.344394 , valid accuracy = 86.593300
epoch =  38, train loss = 0.199388 , train accuracy = 91.827324 , valid loss = 0.452072 , valid accuracy = 83.591797
epoch =  39, train loss = 0.181043 , train accuracy = 92.727371 , valid loss = 0.395017 , valid accuracy = 84.792397
epoch =  40, train loss = 0.174949 , train accuracy = 92.994057 , valid loss = 0.366413 , valid accuracy = 86.143074
epoch =  41, train loss = 0.175031 , train accuracy = 93.021835 , valid loss = 0.403222 , valid accuracy = 83.741875
epoch =  42, train loss = 0.164570 , train accuracy = 93.505196 , valid loss = 0.503415 , valid accuracy = 83.841919
epoch =  43, train loss = 0.158626 , train accuracy = 93.805214 , valid loss = 0.407118 , valid accuracy = 85.292648
epoch =  44, train loss = 0.148377 , train accuracy = 94.288574 , valid loss = 0.541897 , valid accuracy = 82.791397
epoch =  45, train loss = 0.142335 , train accuracy = 94.394135 , valid loss = 0.404406 , valid accuracy = 85.192596
epoch =  46, train loss = 0.130768 , train accuracy = 94.827492 , valid loss = 0.445834 , valid accuracy = 86.043022
epoch =  47, train loss = 0.127403 , train accuracy = 95.077507 , valid loss = 0.518060 , valid accuracy = 83.941971
epoch =  48, train loss = 0.125306 , train accuracy = 95.277519 , valid loss = 0.510292 , valid accuracy = 83.341667
Early stopping
Time required =  3443.366  s 

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}57}]:} <IPython.lib.display.Audio object>
\end{Verbatim}
            
    \subsubsection{If necessary,}\label{if-necessary}

you can use this code to retrieve a particular save state. This should
be used to retrieve the state just before overfitting happens.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}185}]:} \PY{n}{want\PYZus{}to\PYZus{}retrieve} \PY{o}{=} \PY{k+kc}{False}
          \PY{n}{indx\PYZus{}to\PYZus{}retrieve} \PY{o}{=} \PY{l+m+mi}{36}    \PY{c+c1}{\PYZsh{} warning : the epochs are shifted by one : index of epoch i+1 is i}
          \PY{k}{if} \PY{n}{want\PYZus{}to\PYZus{}retrieve} \PY{p}{:}
              \PY{n}{mynet} \PY{o}{=} \PY{n}{VGGClassifier}\PY{p}{(}\PY{p}{)}
              \PY{n}{mynet}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n}{net1\PYZus{}state\PYZus{}dict\PYZus{}list}\PY{p}{[}\PY{n}{indx\PYZus{}to\PYZus{}retrieve}\PY{p}{]}\PY{p}{)}
              \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{mynet}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\end{Verbatim}

    \paragraph{When we runned the code}\label{when-we-runned-the-code}

The training had reached an early stop at epoch 48 after 57m23s. We
retrieved the state of the model at index 36 (epoch 37), just before
obvious overfitting. This state was achieving a minimum in the average
loss and had close to maximum accuracy over the validation dataset. This
is what we had :\\
epoch = 37, train loss = 0.202138 , train accuracy = 91.838432 , valid
loss = 0.344394 , valid accuracy = 86.593300

    \paragraph{Confidence intervals}\label{confidence-intervals}

Now that we have a single value (y=86.59\%) for the accuracy on the
validation dataset (of size n=1999) we would want to build a 95\%
confidence interval for the probability of finding the good label.
Here's how :\\
- Let x\_i be 0 if the net finds the good label for picture i, 0
otherwise\\
- y\_n = sum(x\_i, for i from 1 to n) is a binomial random variable with
parameters n= validation dataset size, p=Pr(x\_i=1) - We use the
Clopper--Pearson confidence interval method to build a 95\% confidence
interval for p knowing y\_n = y

n = 1999; x = 0.8659*n; alpha1 = x; beta1 = n - x + 1; alpha2 = x + 1;
beta2 = n - x; uinf = 0.025; usup = 1 - 0.025; vinf =
N{[}InverseCDF{[}BetaDistribution{[}alpha1, beta1{]}, uinf{]}{]} vsup =
N{[}InverseCDF{[}BetaDistribution{[}alpha2, beta2{]}, usup{]}{]}

And it outputs the following 95\% confidence interval for p (vinf =
0.850175, vsup = 0.880543).

That means that that we should be doing fine on the test dataset (the
kaggle submission) assuming that the probability 'p' of finding the good
label for one sample is the same for the validation and the test
dataset. This assumption may not be true, but knowing that this interval
does not contain 75\% is a good thing.

    \subsubsection{Plot accuracy and loss on the training and validation
dataset}\label{plot-accuracy-and-loss-on-the-training-and-validation-dataset}

and save the result

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{n}{want\PYZus{}log}       \PY{o}{=} \PY{k+kc}{False}
         \PY{n}{early\PYZus{}stop}     \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{early\PYZus{}stop\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{47}
         \PY{n}{title}        \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{VGGClassifier with medium\PYZhy{}low data augmentation, lr = }\PY{l+s+si}{\PYZob{}lr\PYZcb{}}\PY{l+s+s2}{, batch size = }\PY{l+s+si}{\PYZob{}bs\PYZcb{}}\PY{l+s+s2}{, regul. param = }\PY{l+s+si}{\PYZob{}rp\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                             \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,}
                             \PY{n}{bs}\PY{o}{=}\PY{n}{train\PYZus{}batch\PYZus{}size}\PY{p}{,}
                             \PY{n}{rp}\PY{o}{=}\PY{n}{regularization}
                         \PY{p}{)} 
         \PY{k}{if} \PY{n}{early\PYZus{}stop} \PY{p}{:}
             \PY{n}{title} \PY{o}{+}\PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Stopped early}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{path\PYZus{}to\PYZus{}save} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/}\PY{l+s+s2}{\PYZdq{}} 
         \PY{n}{filename}     \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{strftime}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{B}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZus{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{p}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{IH}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{MM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n}{plot\PYZus{}1d\PYZus{}acc\PYZus{}and\PYZus{}loss}\PY{p}{(}\PY{n}{net1}\PY{p}{,} \PY{n}{accuracy1}\PY{p}{[}\PY{p}{:}\PY{n}{early\PYZus{}stop\PYZus{}idx}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{avg\PYZus{}loss1}\PY{p}{[}\PY{p}{:}\PY{n}{early\PYZus{}stop\PYZus{}idx}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{path\PYZus{}to\PYZus{}save}\PY{p}{,} \PY{n}{filename}\PY{p}{,} \PY{n}{title}\PY{p}{,} \PY{n}{want\PYZus{}log}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook :}
         \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February15\PYZus{}PM08H58M.png}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_49_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Finding good
hyper-parameters}\label{finding-good-hyper-parameters}

\paragraph{Search for the right model}\label{search-for-the-right-model}

We could considered the following two respects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Architectrual decisions: those that change the structure of the model
  and its programming structs such as the number of layers, the size of
  hidden layers, non-linearities for activations, kernel parameters
  (e.g., its size and stride), initialization method, etc.
\item
  Tuning iteration hyper-parameters: which is usually done with grid
  search or random search \textgreater{} * mini-batch size
  \textgreater{} * learning rate
\end{enumerate}

Between the two above, the architectrual decisions are more expensive to
implement. Mindful of our recource limitation, we tried several
architectures and comparing their validation errors brought, and we
ended up choosing Classifier 5 over the rest.

As for the hyper-parameters search, we did not want to use grid search,
because it amounts to search for too few point in each individual
dimension. i.e. the cardinality of the projection of the points used in
each dimension is significantly lower than the total number of points
evaluated.\\
In order to efficiently look for parameters, we opted for a low
discrepency deterministic (so called quasi-random) sequence called sobol
sequence that ensures a lower discrepancy than a true random (big holes
in the resulting samples).

    Here's what it looks like.\\
We define one class that wrap a sobol sequence.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}123}]:} \PY{k}{class} \PY{n+nc}{HyperParameterSequence}\PY{p}{(}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
          \PY{l+s+sd}{    n\PYZhy{}dimensional Sobol sequence :}
          \PY{l+s+sd}{        \PYZhy{} starting\PYZus{}point : the point in the (infinite) sobol sequence at which our search will begin}
          \PY{l+s+sd}{                           for reproductibility, this number has to be remembered}
          \PY{l+s+sd}{        \PYZhy{} nb\PYZus{}points      : number of consecutive points of the sequence we use for evaluation}
          \PY{l+s+sd}{        \PYZhy{} dim            : number of dimension of the search}
          \PY{l+s+sd}{        \PYZhy{} c\PYZus{}interval     : list of lists each of the form [lower bound, upper bound] for each dimension}
          \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{starting\PYZus{}point}\PY{p}{,}\PY{n}{nb\PYZus{}points}\PY{p}{,}\PY{n}{dim}\PY{p}{,}\PY{n}{c\PYZus{}interval}\PY{p}{)}\PY{p}{:}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{starting\PYZus{}point} \PY{o}{=} \PY{n}{starting\PYZus{}point}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nb\PYZus{}points}      \PY{o}{=} \PY{n}{nb\PYZus{}points}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}            \PY{o}{=} \PY{n}{dim}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}min} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}max} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{)}
                  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{)} \PY{p}{:}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}min}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{c\PYZus{}interval}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}max}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{c\PYZus{}interval}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
                      
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seq} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{p}{(}\PY{n}{nb\PYZus{}points}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
                  \PY{n}{start} \PY{o}{=} \PY{n}{starting\PYZus{}point}
                  \PY{n}{end}   \PY{o}{=} \PY{n}{start} \PY{o}{+} \PY{n}{nb\PYZus{}points}
                  \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{start}\PY{p}{,}\PY{n}{end}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{p}{:}
                      \PY{n}{hyperparam\PYZus{}point} \PY{p}{,}\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{sobol\PYZus{}seq}\PY{o}{.}\PY{n}{i4\PYZus{}sobol}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{,}\PY{n}{j}\PY{p}{)}
                      \PY{c+c1}{\PYZsh{} take the point in the unitary cube and map it to the desired box}
                      \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}\PY{p}{)} \PY{p}{:}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seq}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{n}{k}\PY{p}{]} \PY{o}{=} \PY{n}{hyperparam\PYZus{}point}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}max}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}min}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{)} \PY{o}{+}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}min}\PY{p}{[}\PY{n}{k}\PY{p}{]}
                  
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nb\PYZus{}points}
              
              \PY{k}{def} \PY{n+nf}{get\PYZus{}dim}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dim}
              
              \PY{k}{def} \PY{n+nf}{get\PYZus{}interval}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{k}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}min}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{c\PYZus{}max}\PY{p}{[}\PY{n}{k}\PY{p}{]}
              
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}getitem\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{idx}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seq}\PY{p}{[}\PY{n}{idx}\PY{p}{]}
              
              \PY{k}{def} \PY{n+nf}{get\PYZus{}sequence}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{seq}
              
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}125}]:} \PY{c+c1}{\PYZsh{} Vizualize a 2d sobol sequence in the desired search box       }
          \PY{n}{starting\PYZus{}point} \PY{o}{=} \PY{l+m+mi}{8030}
          \PY{n}{nb\PYZus{}points}      \PY{o}{=} \PY{l+m+mi}{20}
          \PY{n}{lr\PYZus{}interval}    \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{]}
          \PY{n}{re\PYZus{}interval}    \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,}\PY{l+m+mf}{0.005}\PY{p}{]}
          \PY{n}{intervals}      \PY{o}{=} \PY{p}{[}\PY{n}{lr\PYZus{}interval}\PY{p}{,} \PY{n}{re\PYZus{}interval}\PY{p}{]}
          \PY{n}{hyper\PYZus{}param\PYZus{}sequence} \PY{o}{=} \PY{n}{HyperParameterSequence}\PY{p}{(}\PY{n}{starting\PYZus{}point}\PY{p}{,}\PY{n}{nb\PYZus{}points}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{intervals}\PY{p}{)}
          \PY{n}{seq} \PY{o}{=} \PY{n}{hyper\PYZus{}param\PYZus{}sequence}\PY{o}{.}\PY{n}{get\PYZus{}sequence}\PY{p}{(}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{seq}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_53_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}126}]:} \PY{c+c1}{\PYZsh{} Vizualize 2 dimension hyper\PYZhy{}planes of a 3d sobol sequence }
          \PY{n}{starting\PYZus{}point} \PY{o}{=} \PY{l+m+mi}{10030}
          \PY{n}{nb\PYZus{}points}      \PY{o}{=} \PY{l+m+mi}{20}
          \PY{n}{lr\PYZus{}interval}    \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{]}
          \PY{n}{bs\PYZus{}interval}    \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{64}\PY{p}{,}\PY{l+m+mi}{2}\PY{o}{*}\PY{l+m+mi}{64}\PY{p}{]}    \PY{c+c1}{\PYZsh{} warning : if this is set too high, you can encounted a Runtime Error: CUDA out of memory }
          \PY{n}{re\PYZus{}interval}    \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.0001}\PY{p}{,}\PY{l+m+mf}{0.005}\PY{p}{]}
          \PY{n}{intervals}      \PY{o}{=} \PY{p}{[}\PY{n}{lr\PYZus{}interval}\PY{p}{,} \PY{n}{bs\PYZus{}interval}\PY{p}{,} \PY{n}{re\PYZus{}interval}\PY{p}{]}
          \PY{n}{hyper\PYZus{}param\PYZus{}sequence} \PY{o}{=} \PY{n}{HyperParameterSequence}\PY{p}{(}\PY{n}{starting\PYZus{}point}\PY{p}{,}\PY{n}{nb\PYZus{}points}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{intervals}\PY{p}{)}
          \PY{n}{seq} \PY{o}{=} \PY{n}{hyper\PYZus{}param\PYZus{}sequence}\PY{o}{.}\PY{n}{get\PYZus{}sequence}\PY{p}{(}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{seq}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{seq}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{seq}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_54_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_54_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{What we plan to do :}\label{what-we-plan-to-do}

To find the most promising combination of learning rates, we use the
following pseudocode:

\paragraph{Pseudocode}\label{pseudocode}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We generate a point in hyper-parameter space\\
\item
  We train an net for k epoch using these hyper-parameters\\
\item
  We pick the net that has the highest accuracy on the validation
  dataset\\
\item
  Continue training this net with the same fixed hyper-parameters
\end{enumerate}

\paragraph{Searching hyper-parameters
space}\label{searching-hyper-parameters-space}

in 3 dimensions :\\
- learning rate\\
- batch size\\
- regularisation parameter

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{nb\PYZus{}epoch}  \PY{o}{=} \PY{l+m+mi}{3}
         \PY{n}{patience}  \PY{o}{=} \PY{l+m+mi}{2}
         \PY{c+c1}{\PYZsh{} current mlp with the best performance on the validation set, on its last epoch}
         \PY{n}{acc\PYZus{}max} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{idx\PYZus{}max} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{c+c1}{\PYZsh{} we save (all) the intermediate state of the model during the learning phase, for each mlp}
         \PY{n}{state\PYZus{}dict\PYZus{}dict} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{p}{)} 
         \PY{n}{valid\PYZus{}batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{64}
         \PY{n}{valid\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{valid\PYZus{}batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{hyperparam\PYZus{}point} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{hyper\PYZus{}param\PYZus{}sequence}\PY{p}{)}\PY{p}{:}  
             \PY{n}{lr}         \PY{o}{=} \PY{n}{hyperparam\PYZus{}point}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{math}\PY{o}{.}\PY{n}{ceil}\PY{p}{(}\PY{n}{hyperparam\PYZus{}point}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} cast to the correct type}
             \PY{n}{regul}      \PY{o}{=} \PY{n}{hyperparam\PYZus{}point}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
             \PY{n}{net\PYZus{}tmp} \PY{o}{=} \PY{n}{VGGClassifier}\PY{p}{(}\PY{p}{)} 
             \PY{n}{net\PYZus{}tmp}\PY{o}{.}\PY{n}{apply}\PY{p}{(} \PY{n}{glorot\PYZus{}init} \PY{p}{)}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{net\PYZus{}tmp}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
             
             \PY{n}{criterion} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{net\PYZus{}tmp}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,} \PY{n}{momentum}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{weight\PYZus{}decay}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
             
             \PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}augm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
             \PY{n}{state\PYZus{}dict\PYZus{}list\PYZus{}tmp} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} we save (all) the intermediate state of the model during the learning phase, for one  mlp}
         
             \PY{c+c1}{\PYZsh{} average loss across epoch}
             \PY{n}{avg\PYZus{}loss\PYZus{}tmp}     \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{nb\PYZus{}epoch}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} accuracy[i, 0 (resp. 1)] is the training (reps. validation) accuracy of the net at epoch i}
             \PY{n}{accuracy\PYZus{}tmp}     \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{nb\PYZus{}epoch}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} print hyper\PYZhy{}parameters }
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{point no. }\PY{l+s+si}{\PYZob{}i\PYZcb{}}\PY{l+s+s2}{, lr = }\PY{l+s+si}{\PYZob{}lr\PYZcb{}}\PY{l+s+s2}{, batch size = }\PY{l+s+si}{\PYZob{}batch\PYZus{}size\PYZcb{}}\PY{l+s+s2}{, regul=}\PY{l+s+si}{\PYZob{}regul\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}
                         \PY{n}{i}\PY{o}{=}\PY{n}{i}\PY{p}{,} 
                         \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,}
                         \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}
                         \PY{n}{regul}\PY{o}{=}\PY{n}{regul}
                 \PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} we dump output to disable sound}
             \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
             \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{training\PYZus{}phase}\PY{p}{(} \PY{n}{net\PYZus{}tmp}\PY{p}{,} \PY{n}{nb\PYZus{}epoch}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{regul}\PY{p}{,} \PY{n}{patience}\PY{p}{,} \PY{n}{avg\PYZus{}loss\PYZus{}tmp}\PY{p}{,} 
                                    \PY{n}{accuracy\PYZus{}tmp}\PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{,} \PY{n}{valid\PYZus{}loader}\PY{p}{,} \PY{n}{state\PYZus{}dict\PYZus{}list\PYZus{}tmp} \PY{p}{)}
             \PY{n}{state\PYZus{}dict\PYZus{}dict}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{lr}\PY{p}{,}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{regul}\PY{p}{]}\PY{p}{,}\PY{n}{state\PYZus{}dict\PYZus{}list\PYZus{}tmp}\PY{p}{,}\PY{n}{avg\PYZus{}loss\PYZus{}tmp}\PY{p}{,}\PY{n}{accuracy\PYZus{}tmp}\PY{p}{]}
             
             
             \PY{n}{valid\PYZus{}accuracy} \PY{o}{=} \PY{n}{accuracy\PYZus{}tmp}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} last validation accuracy}
             \PY{k}{if} \PY{n}{valid\PYZus{}accuracy} \PY{o}{\PYZgt{}} \PY{n}{acc\PYZus{}max} \PY{p}{:}
                 \PY{n}{acc\PYZus{}max} \PY{o}{=} \PY{n}{valid\PYZus{}accuracy}
                 \PY{n}{idx\PYZus{}max} \PY{o}{=} \PY{n}{i}
             \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{best net found : }\PY{l+s+si}{\PYZob{}i\PYZcb{}}\PY{l+s+s2}{ , with validation accuracy = }\PY{l+s+si}{\PYZob{}va\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i}\PY{o}{=}\PY{n}{idx\PYZus{}max}\PY{p}{,}\PY{n}{va}\PY{o}{=}\PY{n}{acc\PYZus{}max}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{k}{if} \PY{n}{want\PYZus{}lound\PYZus{}warning} \PY{p}{:}
             \PY{n}{Audio}\PY{p}{(}\PY{n}{wave}\PY{p}{,} \PY{n}{rate}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,} \PY{n}{autoplay}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
point no. 0, lr = 0.006178131103515626, batch size = 76, regul=0.001434759521484375
epoch =   1, train loss = 0.685866 , train accuracy = 54.664146 , valid loss = 0.674845 , valid accuracy = 59.629814
epoch =   2, train loss = 0.672871 , train accuracy = 58.781044 , valid loss = 0.679346 , valid accuracy = 56.528263
epoch =   3, train loss = 0.659360 , train accuracy = 61.258961 , valid loss = 0.662868 , valid accuracy = 59.479740
Finished Training
Time required =  502.1490625  s 
point no. 1, lr = 0.001228131103515625, batch size = 108, regul=0.0038847595214843746
epoch =   1, train loss = 0.690818 , train accuracy = 54.180786 , valid loss = 0.689068 , valid accuracy = 56.028015
epoch =   2, train loss = 0.686327 , train accuracy = 57.325405 , valid loss = 0.686039 , valid accuracy = 54.627316
epoch =   3, train loss = 0.681139 , train accuracy = 58.175453 , valid loss = 0.680879 , valid accuracy = 55.727863
Finished Training
Time required =  180.66470312500002  s 
point no. 2, lr = 0.0009187561035156251, batch size = 74, regul=0.003425384521484375
epoch =   1, train loss = 0.691839 , train accuracy = 52.291794 , valid loss = 0.690343 , valid accuracy = 51.425713
epoch =   2, train loss = 0.686875 , train accuracy = 57.103172 , valid loss = 0.685423 , valid accuracy = 57.528763
epoch =   3, train loss = 0.681454 , train accuracy = 58.369911 , valid loss = 0.686156 , valid accuracy = 53.676838
Finished Training
Time required =  225.22684375  s 
point no. 3, lr = 0.005868756103515626, batch size = 106, regul=0.000975384521484375
epoch =   1, train loss = 0.686939 , train accuracy = 54.975277 , valid loss = 0.677549 , valid accuracy = 55.677837
epoch =   2, train loss = 0.674015 , train accuracy = 58.347687 , valid loss = 0.657008 , valid accuracy = 64.732368
epoch =   3, train loss = 0.661800 , train accuracy = 60.714485 , valid loss = 0.639054 , valid accuracy = 64.632317
Finished Training
Time required =  216.96814062500002  s 
point no. 4, lr = 0.008343756103515626, batch size = 90, regul=0.002200384521484375
epoch =   1, train loss = 0.686835 , train accuracy = 54.597477 , valid loss = 0.673481 , valid accuracy = 60.630314
epoch =   2, train loss = 0.672708 , train accuracy = 58.492138 , valid loss = 0.654444 , valid accuracy = 62.981491
epoch =   3, train loss = 0.663005 , train accuracy = 60.292240 , valid loss = 0.638610 , valid accuracy = 64.232117
Finished Training
Time required =  208.413671875  s 
point no. 5, lr = 0.0033937561035156253, batch size = 122, regul=0.004650384521484375
epoch =   1, train loss = 0.689654 , train accuracy = 54.214123 , valid loss = 0.686127 , valid accuracy = 56.678341
epoch =   2, train loss = 0.680568 , train accuracy = 57.942108 , valid loss = 0.681511 , valid accuracy = 54.527264
epoch =   3, train loss = 0.672449 , train accuracy = 58.486584 , valid loss = 0.663968 , valid accuracy = 58.979488
Finished Training
Time required =  193.61839062500002  s 
point no. 6, lr = 0.004631256103515626, batch size = 82, regul=0.000362884521484375
epoch =   1, train loss = 0.685447 , train accuracy = 55.203068 , valid loss = 0.706556 , valid accuracy = 49.974987
epoch =   2, train loss = 0.671755 , train accuracy = 58.731041 , valid loss = 0.663824 , valid accuracy = 57.628815
epoch =   3, train loss = 0.661130 , train accuracy = 61.131172 , valid loss = 0.652674 , valid accuracy = 61.330666
Finished Training
Time required =  290.4695625  s 
point no. 7, lr = 0.009581256103515625, batch size = 114, regul=0.0028128845214843747
epoch =   1, train loss = 0.687758 , train accuracy = 54.547474 , valid loss = 0.682240 , valid accuracy = 52.676338
epoch =   2, train loss = 0.676364 , train accuracy = 58.164341 , valid loss = 0.673263 , valid accuracy = 58.329166
epoch =   3, train loss = 0.663117 , train accuracy = 60.364464 , valid loss = 0.671408 , valid accuracy = 58.679340
Finished Training
Time required =  184.215546875  s 
point no. 8, lr = 0.007106256103515626, batch size = 66, regul=0.004037884521484375
epoch =   1, train loss = 0.683719 , train accuracy = 56.030891 , valid loss = 0.699001 , valid accuracy = 50.575287
epoch =   2, train loss = 0.667391 , train accuracy = 59.414413 , valid loss = 0.680178 , valid accuracy = 52.576286
epoch =   3, train loss = 0.649160 , train accuracy = 62.381245 , valid loss = 0.628365 , valid accuracy = 66.183090
Finished Training
Time required =  239.113875  s 
point no. 9, lr = 0.002156256103515625, batch size = 98, regul=0.001587884521484375
epoch =   1, train loss = 0.688308 , train accuracy = 54.041893 , valid loss = 0.682598 , valid accuracy = 59.229614
epoch =   2, train loss = 0.676788 , train accuracy = 58.019890 , valid loss = 0.703180 , valid accuracy = 52.176086
epoch =   3, train loss = 0.670578 , train accuracy = 58.942162 , valid loss = 0.702606 , valid accuracy = 53.226612
Finished Training
Time required =  204.82440625  s 
point no. 10, lr = 0.0015375061035156252, batch size = 94, regul=0.004956634521484375
epoch =   1, train loss = 0.690931 , train accuracy = 52.930717 , valid loss = 0.690067 , valid accuracy = 53.876938
epoch =   2, train loss = 0.687426 , train accuracy = 56.203121 , valid loss = 0.685838 , valid accuracy = 57.728863
epoch =   3, train loss = 0.682077 , train accuracy = 58.392132 , valid loss = 0.678740 , valid accuracy = 58.929466
Finished Training
Time required =  201.746234375  s 
point no. 11, lr = 0.006487506103515625, batch size = 126, regul=0.0025066345214843746
epoch =   1, train loss = 0.687137 , train accuracy = 54.553032 , valid loss = 0.677833 , valid accuracy = 60.130066
epoch =   2, train loss = 0.679178 , train accuracy = 57.225403 , valid loss = 0.671482 , valid accuracy = 59.679840
epoch =   3, train loss = 0.668690 , train accuracy = 59.358852 , valid loss = 0.654090 , valid accuracy = 61.730865
Finished Training
Time required =  391.17971875  s 
point no. 12, lr = 0.008962506103515625, batch size = 78, regul=0.001281634521484375
epoch =   1, train loss = 0.684106 , train accuracy = 55.325294 , valid loss = 0.676528 , valid accuracy = 56.678341
epoch =   2, train loss = 0.673142 , train accuracy = 58.819935 , valid loss = 0.669340 , valid accuracy = 58.029015
epoch =   3, train loss = 0.658953 , train accuracy = 60.964497 , valid loss = 0.638885 , valid accuracy = 66.833420
Finished Training
Time required =  436.73053125  s 
point no. 13, lr = 0.004012506103515626, batch size = 110, regul=0.0037316345214843745
epoch =   1, train loss = 0.687297 , train accuracy = 54.875271 , valid loss = 0.692994 , valid accuracy = 51.575787
epoch =   2, train loss = 0.676870 , train accuracy = 57.925442 , valid loss = 0.689186 , valid accuracy = 54.127064
epoch =   3, train loss = 0.667833 , train accuracy = 59.769989 , valid loss = 0.663677 , valid accuracy = 57.978989
Finished Training
Time required =  446.13125  s 
point no. 14, lr = 0.002775006103515625, batch size = 70, regul=0.001894134521484375
epoch =   1, train loss = 0.684204 , train accuracy = 55.814213 , valid loss = 0.736925 , valid accuracy = 49.474739
epoch =   2, train loss = 0.669186 , train accuracy = 59.564419 , valid loss = 0.670535 , valid accuracy = 59.029514
epoch =   3, train loss = 0.659597 , train accuracy = 60.447803 , valid loss = 0.634700 , valid accuracy = 64.882439
Finished Training
Time required =  251.426875  s 
point no. 15, lr = 0.007725006103515626, batch size = 102, regul=0.004344134521484375
epoch =   1, train loss = 0.685369 , train accuracy = 55.236401 , valid loss = 0.679365 , valid accuracy = 53.926964
epoch =   2, train loss = 0.673450 , train accuracy = 59.003277 , valid loss = 0.657237 , valid accuracy = 61.180592
epoch =   3, train loss = 0.662159 , train accuracy = 60.636703 , valid loss = 0.699421 , valid accuracy = 54.177090
Finished Training
Time required =  574.60675  s 
point no. 16, lr = 0.0052500061035156255, batch size = 86, regul=0.0031191345214843747
epoch =   1, train loss = 0.686815 , train accuracy = 54.708595 , valid loss = 0.695554 , valid accuracy = 51.675838
epoch =   2, train loss = 0.675823 , train accuracy = 57.792099 , valid loss = 0.661089 , valid accuracy = 62.981491
epoch =   3, train loss = 0.661904 , train accuracy = 60.208900 , valid loss = 0.654032 , valid accuracy = 64.082039
Finished Training
Time required =  417.813625  s 
point no. 17, lr = 0.000300006103515625, batch size = 118, regul=0.000669134521484375
epoch =   1, train loss = 0.693041 , train accuracy = 51.486195 , valid loss = 0.692249 , valid accuracy = 54.177090
epoch =   2, train loss = 0.691775 , train accuracy = 55.030834 , valid loss = 0.691789 , valid accuracy = 51.775887
epoch =   3, train loss = 0.691060 , train accuracy = 53.930775 , valid loss = 0.691050 , valid accuracy = 53.426712
Finished Training
Time required =  299.41390625  s 
point no. 18, lr = 0.000377349853515625, batch size = 96, regul=0.0030808532714843746
epoch =   1, train loss = 0.692339 , train accuracy = 52.419579 , valid loss = 0.691695 , valid accuracy = 51.625813
epoch =   2, train loss = 0.690665 , train accuracy = 54.308571 , valid loss = 0.690459 , valid accuracy = 53.776890
epoch =   3, train loss = 0.689216 , train accuracy = 56.569809 , valid loss = 0.689074 , valid accuracy = 56.128063
Finished Training
Time required =  601.55975  s 
point no. 19, lr = 0.005327349853515626, batch size = 128, regul=0.000630853271484375
epoch =   1, train loss = 0.687477 , train accuracy = 54.297462 , valid loss = 0.678247 , valid accuracy = 58.879440
epoch =   2, train loss = 0.679432 , train accuracy = 56.742043 , valid loss = 0.668411 , valid accuracy = 61.380692
epoch =   3, train loss = 0.669451 , train accuracy = 59.253292 , valid loss = 0.651768 , valid accuracy = 64.232117
Finished Training
Time required =  219.72664062500002  s 
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
best net found : 12 , with validation accuracy = 66.83341979980469

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:} <IPython.lib.display.Audio object>
\end{Verbatim}
            
    \subsubsection{Display the result of the
search}\label{display-the-result-of-the-search}

In the following plot, the bigger the area of the point, the higher is
the accuracy of the model corresponding to its coordinated
hyperparameters.

 Big points are good, small points are bad.\\
We scale the area of the points to make the results easily
understandable. 

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}127}]:} \PY{n}{want\PYZus{}log}     \PY{o}{=} \PY{k+kc}{True}
          \PY{n}{path\PYZus{}to\PYZus{}save} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/}\PY{l+s+s2}{\PYZdq{}} 
          \PY{n}{filename}     \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{strftime}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{B}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZus{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{p}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{IH}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{MM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{title}        \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{VGGClassifier, using medium\PYZhy{}low data augmentation and 3 epochs}\PY{l+s+s2}{\PYZdq{}}
          
          \PY{c+c1}{\PYZsh{} retrieve the sequence used for the search  }
          \PY{n}{hyper\PYZus{}param}  \PY{o}{=} \PY{n}{hyper\PYZus{}param\PYZus{}sequence}\PY{o}{.}\PY{n}{get\PYZus{}sequence}\PY{p}{(}\PY{p}{)}  
          \PY{n}{N}            \PY{o}{=} \PY{n}{hyper\PYZus{}param\PYZus{}sequence}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} define hyper\PYZhy{}planes to display}
          \PY{n}{hyper\PYZus{}param\PYZus{}plane} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{N}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
          \PY{n}{hyper\PYZus{}param\PYZus{}plane}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{hyper\PYZus{}param}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}
          \PY{n}{hyper\PYZus{}param\PYZus{}plane}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{hyper\PYZus{}param}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}
          \PY{n}{hyper\PYZus{}param\PYZus{}plane}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{hyper\PYZus{}param}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}
          \PY{n}{axis\PYZus{}label}   \PY{o}{=} \PY{p}{[}
                  \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{learning rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{batch size}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
                  \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{learning rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{regularization parameter}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,}
                  \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{batch size}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{regularization parameter}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
              \PY{p}{]}
          \PY{n}{\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{N}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} retrieve previously measured accuracy }
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{:}
              \PY{c+c1}{\PYZsh{} [lr,batch\PYZus{}size],state\PYZus{}dict\PYZus{}list\PYZus{}tmp,avg\PYZus{}loss\PYZus{}tmp,accuracy\PYZus{}tmp}
              \PY{n}{\PYZus{}}\PY{p}{,}\PY{n}{\PYZus{}}\PY{p}{,}\PY{n}{\PYZus{}}\PY{p}{,}\PY{n}{acc} \PY{o}{=} \PY{n}{state\PYZus{}dict\PYZus{}dict}\PY{p}{[}\PY{n}{i}\PY{p}{]}
              \PY{n}{\PYZus{}val}\PY{p}{[}\PY{n}{i}\PY{p}{]}   \PY{o}{=} \PY{n}{acc}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}  
          
          \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
              \PY{n}{plot\PYZus{}accuracy\PYZus{}2d}\PY{p}{(}\PY{n}{\PYZus{}val}\PY{p}{,}\PY{n}{hyper\PYZus{}param\PYZus{}plane}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{,}\PY{n}{path\PYZus{}to\PYZus{}save}\PY{p}{,}\PY{n}{filename} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}k\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{k}\PY{o}{=}\PY{n}{k}\PY{p}{)}\PY{p}{,}
                               \PY{n}{title}\PY{p}{,}\PY{n}{axis\PYZus{}label}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{,}\PY{n}{want\PYZus{}log}\PY{p}{,}\PY{n}{scaling}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_58_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_58_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_58_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Past stuff}\label{past-stuff}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}165}]:} \PY{c+c1}{\PYZsh{} using Classifier5d}
          \PY{c+c1}{\PYZsh{} using high data augmentation we found lr = 0.00075, batch size = 512}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February09\PYZus{}PM09H02M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_60_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Our results}\label{our-results}

This next section is composed of a list of plots in group of two:\\
- The first shows the result of the search in the hyper-parameter
space\\
- The second shows the result of taking those hyper-parameters and
training a model with them

The discussion comes after the plots.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}163}]:} \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook with high data augmentation :}
          \PY{c+c1}{\PYZsh{} and :}
          \PY{c+c1}{\PYZsh{} starting\PYZus{}point = 4030}
          \PY{c+c1}{\PYZsh{} nb\PYZus{}points      = 20}
          \PY{c+c1}{\PYZsh{} lr\PYZus{}interval    = [0.01,0.00001]}
          \PY{c+c1}{\PYZsh{} bs\PYZus{}interval    = [16,80]\PYZsh{} Vizualize a 2d sobol sequence in the desired search box   }
          \PY{c+c1}{\PYZsh{} using high data augmentation we found lr = 0.0020221459960937504, batch size = 54}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}AM12H32M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook :}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}AM01H03M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_62_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_62_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}175}]:} \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook with medium data augmentation :}
          \PY{c+c1}{\PYZsh{} and :}
          \PY{c+c1}{\PYZsh{} starting\PYZus{}point = 3030}
          \PY{c+c1}{\PYZsh{} nb\PYZus{}points      = 20}
          \PY{c+c1}{\PYZsh{} lr\PYZus{}interval    = [0.002,0.00005]}
          \PY{c+c1}{\PYZsh{} bs\PYZus{}interval    = [16,2*64]}
          \PY{c+c1}{\PYZsh{} using Classifier5, no regularization and medium  data augmentation : }
          \PY{c+c1}{\PYZsh{} lr = 0.0005646362304687499, batch size = 18}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February13\PYZus{}PM10H33M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February13\PYZus{}PM11H49M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_63_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_63_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}177}]:} \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook with low data augmentation :}
          \PY{c+c1}{\PYZsh{} and :}
          \PY{c+c1}{\PYZsh{} starting\PYZus{}point = 2030}
          \PY{c+c1}{\PYZsh{} nb\PYZus{}points      = 20}
          \PY{c+c1}{\PYZsh{} lr\PYZus{}interval    = [0.00001,0.01]}
          \PY{c+c1}{\PYZsh{} bs\PYZus{}interval    = [16,64]}
          \PY{c+c1}{\PYZsh{} using Classifier5, no regularization and low data augmentation : }
          \PY{c+c1}{\PYZsh{} lr = 0.0003392602539062500, batch size = 21}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}PM01H40M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_64_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}176}]:} \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook with medium\PYZhy{}low data augmentation :}
          \PY{c+c1}{\PYZsh{} and :}
          \PY{c+c1}{\PYZsh{} starting\PYZus{}point = 2030}
          \PY{c+c1}{\PYZsh{} nb\PYZus{}points      = 20}
          \PY{c+c1}{\PYZsh{} lr\PYZus{}interval    = [0.0001,0.01]}
          \PY{c+c1}{\PYZsh{} re\PYZus{}interval    = [0.0001,0.03]}
          \PY{c+c1}{\PYZsh{} bs\PYZus{}interval    = [20,20]}
          \PY{c+c1}{\PYZsh{} using Classifier5, regularization and medium\PYZhy{}low data augmentation :  HITLER}
          \PY{c+c1}{\PYZsh{} lr = 0.00129663, batch size = 20, regul = 0.00378247 }
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}PM10H18M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}PM11H48M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_65_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_65_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}173}]:} \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook with low data augmentation :}
          \PY{c+c1}{\PYZsh{} and :}
          \PY{c+c1}{\PYZsh{} starting\PYZus{}point = 5030}
          \PY{c+c1}{\PYZsh{} nb\PYZus{}points      = 20}
          \PY{c+c1}{\PYZsh{} lr\PYZus{}interval    = [0.00001,0.01]}
          \PY{c+c1}{\PYZsh{} re\PYZus{}interval    = [0.0001,0.01]}
          \PY{c+c1}{\PYZsh{} bs\PYZus{}interval    = [16,64]}
          \PY{c+c1}{\PYZsh{} using Classifier5, no regularization and low data augmentation : }
          \PY{c+c1}{\PYZsh{} lr = 0.00236848, batch size = 20, regul = 0.00522644 }
          \PY{c+c1}{\PYZsh{} lr = 0.0003392602539062500, batch size = 21}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}PM03H35M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}PM05H01M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_66_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_66_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}172}]:} \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook with Classifier7 and medium data augmentation :}
          \PY{c+c1}{\PYZsh{} and :}
          \PY{c+c1}{\PYZsh{} starting\PYZus{}point = 6030}
          \PY{c+c1}{\PYZsh{} nb\PYZus{}points      = 20}
          \PY{c+c1}{\PYZsh{} lr\PYZus{}interval    = [0.000001,0.005]}
          \PY{c+c1}{\PYZsh{} re\PYZus{}interval    = [0.00001,0.1]}
          \PY{c+c1}{\PYZsh{} bs\PYZus{}interval    = 20}
          \PY{c+c1}{\PYZsh{} using Classifier7, regularisation and medium data augmentation : }
          \PY{c+c1}{\PYZsh{} lr = 0.00110307, batch size = 20, regul = 0.02835189 }
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}PM08H43M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February14\PYZus{}PM09H17M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_67_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_67_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}165}]:} \PY{c+c1}{\PYZsh{} Here\PYZsq{}s what we add when we runned the notebook with VGGClassifier and medium\PYZhy{}low data augmentation :}
          \PY{c+c1}{\PYZsh{} and :}
          \PY{c+c1}{\PYZsh{} starting\PYZus{}point = 10030}
          \PY{c+c1}{\PYZsh{} nb\PYZus{}points      = 20}
          \PY{c+c1}{\PYZsh{} lr\PYZus{}interval    = [0.0001, 0.01]}
          \PY{c+c1}{\PYZsh{} bs\PYZus{}interval    = [  1*64, 2*64]   }
          \PY{c+c1}{\PYZsh{} re\PYZus{}interval    = [0.0001,0.005]}
          \PY{c+c1}{\PYZsh{} using VGGClassifier, regularisation, using med\PYZhy{}low data augmentation we found :}
          \PY{c+c1}{\PYZsh{} lr = 0.008962506103515625 , batch size = 78, regul = 0.00128163}
          \PY{c+c1}{\PYZsh{} these 3 plots each represent a different hyper\PYZhy{}plane of the search}
          \PY{c+c1}{\PYZsh{} loading\PYZus{}path = \PYZdq{}./output/2019February15\PYZus{}PM10H53M0.png\PYZdq{}}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February15\PYZus{}PM10H53M1.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{c+c1}{\PYZsh{} loading\PYZus{}path = \PYZdq{}./output/2019February15\PYZus{}PM10H53M2.png\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} the minimum loss on the validation dataset is at epoch 34}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./output/2019February15\PYZus{}PM08H58M.png}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{display}\PY{p}{(}\PY{n}{IPython}\PY{o}{.}\PY{n}{display}\PY{o}{.}\PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_68_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_68_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Our interpretation}\label{our-interpretation}

\paragraph{Compare different hyperparameter
settings}\label{compare-different-hyperparameter-settings}

The best type of data augmentation was determined using the Classifier5
model (with \textasciitilde{}2 millions params). In order to be able to
find what type of data augmentation works best, we had to settle to make
the search using a 'small-but-not-too-small' model and stop at epoch 50.
We could not afford to make the search using a very large model (i.e.
VGGClassifier).

The plots show that the best performance (without regularization) was
found with medium data augmentation. Higher than that, the net had not
enough capacity to learn with the training dataset and was learning
slowly. Lower than that, the net was quickly reaching a good accuracy on
the training dataset and was overfitting.

So, we decided to implement regularization and compare low and
medium-low data augmentation. Regularization improved the model
performance by reducing the gap between training and validation dataset
accuracy. Also, the gap between training and validation dataset accuracy
was found reduced the most using medium-low data augmentation.

Then, we tried to use medium data augmentation with a bigger model :
Classifier7 which has \textasciitilde{}6 millions params. We wanted to
see if its larger capacity would enable it to overfit the training
dataset. The answer was negative so we concluded that medium data
augmentation was making the learning task too difficult and we had to
settle for a milder augmentation intensity for the next big run.
Medium-low data augmentation was our best candidate.

The default VGGClassifier model (defined above) has
\textasciitilde{}13millions parameters. We picked a good combinason of
hyper-parameters for it using the same search method as before. Then, we
trained it with those hyper-parameters until it early stopped at epoch
47. We had saved its state at each epoch of the training and we loaded
the state that was both minimizing average loss and close-to-maximizing
accuracy for the validation dataset. The plots show that this model
achieved the best accuracy over the validation dataset.

\paragraph{Plot the training error and validation error curves, along
with the training and validationlosses. Comment on
them.}\label{plot-the-training-error-and-validation-error-curves-along-with-the-training-and-validationlosses.-comment-on-them.}

\paragraph{Report the final results of performance on your validation
set
:}\label{report-the-final-results-of-performance-on-your-validation-set}

Our final results are : - training dataset accuracy = 91.838432 \%\\
- validation dataset accuracy = 86.593300 \%

With the following 95\% confidence interval (c.i.) for the probability
of finding the good label on the validation dataset (85.01\%, 88.05\%).
The computation of this c.i. was explained earlier on the notebook.
Seeing the gap between the two datasets accuracy indicates that we have
to expect a lower accuracy on the test dataset. Especially since the
model used for submission to kaggle was retrieved because of its good
performance on the validation dataset.

\paragraph{How does your validation performance compare to the test set
performance (that you can only get in
Kaggle)}\label{how-does-your-validation-performance-compare-to-the-test-set-performance-that-you-can-only-get-in-kaggle}

On kaggle, we obtain 86.514\% accuracy on the test set. This is
numerically very close to the validation dataset accuracy and falls
inside the 95\% confidence interval for the probability of finding the
good label on the validation dataset.

\paragraph{What techniques (you did not implement) could be useful to
improve the validation
performance.}\label{what-techniques-you-did-not-implement-could-be-useful-to-improve-the-validation-performance.}

What we should talk about : - dropouts\\
- more data augmentation together with a bigger capacity model\\
- ...\\
- There are different techniques that are useful to explore further such
as batch normalization and transfer learning.\\
- We also want to suggest that the model can become more robust if we
could use some regularizations and learning rate tuning as the learning
progresses.

    \section{Miscellaneous}\label{miscellaneous}

\paragraph{Aside from quantitative results, also include some visual
analysis such as
:}\label{aside-from-quantitative-results-also-include-some-visual-analysis-such-as}

\begin{itemize}
\tightlist
\item
  visualizing the feature maps or kernels, or\\
\item
  showing examples where the images are :

  \begin{itemize}
  \item
    \begin{enumerate}
    \def\labelenumi{(\alph{enumi})}
    \tightlist
    \item
      clearly misclassified and\\
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{(\alph{enumi})}
    \setcounter{enumi}{1}
    \tightlist
    \item
      where the classifier predicts around 50\% on both classes.
    \end{enumerate}
  \end{itemize}
\end{itemize}

\paragraph{Explain yourobservation and/or suggest any improvements you
think may
help}\label{explain-yourobservation-andor-suggest-any-improvements-you-think-may-help}

    \subsubsection{Answer}\label{answer}

In order to measure the model's performance we run the model on the
validation set again and measure how confident (at what probability) the
model predicts correctly or misclassifies.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}138}]:} \PY{c+c1}{\PYZsh{} For this part, the neural network should be called : mynet}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}136}]:} \PY{c+c1}{\PYZsh{} set the number of pictures to display}
          \PY{n}{max\PYZus{}nb\PYZus{}to\PYZus{}display} \PY{o}{=} \PY{l+m+mi}{32}
          
          \PY{n}{worst\PYZus{}false\PYZus{}}  \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{best\PYZus{}correct\PYZus{}} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{positivehisto} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{negativehisto} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{k+kn}{from} \PY{n+nn}{heapq} \PY{k}{import} \PY{o}{*}
          
          \PY{c+c1}{\PYZsh{} class introduce to store 2 elements and only use the first for comparison purpose}
          \PY{k}{class} \PY{n+nc}{CostAndValue}\PY{p}{:}
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{cost}\PY{p}{,} \PY{n}{value}\PY{p}{)}\PY{p}{:}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost}  \PY{o}{=} \PY{n}{cost}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{value} \PY{o}{=} \PY{n}{value}
          
              \PY{c+c1}{\PYZsh{} do not compare values}
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}lt\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{other}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost} \PY{o}{\PYZlt{}} \PY{n}{other}\PY{o}{.}\PY{n}{cost}
              
              \PY{k}{def} \PY{n+nf}{get\PYZus{}cost\PYZus{}and\PYZus{}value}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)} \PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{cost}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{value}    
              
          \PY{c+c1}{\PYZsh{} }
          \PY{k}{def} \PY{n+nf}{confidence}\PY{p}{(}\PY{n}{proba}\PY{p}{)}\PY{p}{:}
              \PY{n}{pred} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{proba}\PY{p}{,}\PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
              \PY{k}{return} \PY{p}{(}\PY{n}{pred}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/} \PY{n}{pred}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{pred}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
          
          \PY{n}{valid\PYZus{}batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{64}
          \PY{n}{valid\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{valid\PYZus{}batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
          
          \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)} \PY{p}{:}
              \PY{k}{for} \PY{n}{img}\PY{p}{,} \PY{n}{lab} \PY{o+ow}{in} \PY{n}{valid\PYZus{}loader}\PY{p}{:}
                  \PY{n}{img} \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                  \PY{n}{lab} \PY{o}{=} \PY{n}{lab}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                  \PY{n}{proba} \PY{o}{=} \PY{n}{mynet}\PY{p}{(}\PY{n}{img}\PY{p}{)}
                  \PY{n}{confid}\PY{p}{,} \PY{n}{pred} \PY{o}{=} \PY{n}{confidence}\PY{p}{(}\PY{n}{proba}\PY{p}{)}
                  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{img}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{p}{:}
                      \PY{n}{img}    \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
                      \PY{n}{lab}    \PY{o}{=} \PY{n}{lab}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
                      \PY{n}{confid} \PY{o}{=} \PY{n}{confid}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
                      \PY{n}{pred}   \PY{o}{=} \PY{n}{pred}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
                          
                      \PY{n}{img\PYZus{}} \PY{o}{=} \PY{n}{img}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                      \PY{n}{c}    \PY{o}{=} \PY{n}{confid}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
                      \PY{k}{if} \PY{n}{pred}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{!=} \PY{n}{lab}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}
                          \PY{n}{elem} \PY{o}{=} \PY{n}{CostAndValue}\PY{p}{(}\PY{n}{c}\PY{p}{,}\PY{n}{img\PYZus{}}\PY{p}{)}
                          \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{worst\PYZus{}false}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}nb\PYZus{}to\PYZus{}display} \PY{p}{:}
                              \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{heappushpop}\PY{p}{(}\PY{n}{worst\PYZus{}false\PYZus{}}\PY{p}{,}\PY{n}{elem}\PY{p}{)}
                          \PY{k}{else}\PY{p}{:}
                              \PY{n}{heappush}\PY{p}{(}\PY{n}{worst\PYZus{}false\PYZus{}}\PY{p}{,}\PY{n}{elem}\PY{p}{)} 
                          \PY{n}{negativehisto}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{c}\PY{p}{)}
                      \PY{k}{else}\PY{p}{:}
                          \PY{n}{elem} \PY{o}{=} \PY{n}{CostAndValue}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{c}\PY{p}{,}\PY{n}{img\PYZus{}}\PY{p}{)}
                          \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{best\PYZus{}correct}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}nb\PYZus{}to\PYZus{}display} \PY{p}{:} \PY{c+c1}{\PYZsh{} all end up having more confidence than 0.99999}
                              \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{heappushpop}\PY{p}{(}\PY{n}{best\PYZus{}correct\PYZus{}}\PY{p}{,}\PY{n}{elem}\PY{p}{)}
                          \PY{k}{else}\PY{p}{:}
                              \PY{n}{heappush}\PY{p}{(}\PY{n}{best\PYZus{}correct\PYZus{}}\PY{p}{,}\PY{n}{elem}\PY{p}{)} 
                          \PY{n}{positivehisto}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{c}\PY{p}{)}
          
          \PY{n}{worst\PYZus{}false}  \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{best\PYZus{}correct} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}nb\PYZus{}to\PYZus{}display}\PY{p}{)} \PY{p}{:}
              \PY{n}{worst\PYZus{}false}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{worst\PYZus{}false\PYZus{}}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{get\PYZus{}cost\PYZus{}and\PYZus{}value}\PY{p}{(}\PY{p}{)}\PY{p}{)}
              \PY{n}{best\PYZus{}correct}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{best\PYZus{}correct\PYZus{}}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{get\PYZus{}cost\PYZus{}and\PYZus{}value}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    Now by plotting the top k=32 most and least confident prediction we can
see that the ones with highest probability are the ones where the animal
is looking directly at the camera and create a more defined face parts
whereas the ones that are highly doubted by the model have less color
contrast on their faces or with the background. As we'll point out in
the kernel, the model seems to be looking for elements in the image
trying to

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}137}]:} \PY{k}{def} \PY{n+nf}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{:}
              \PY{n}{npimg} \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}  \PY{o}{/} \PY{l+m+mi}{255}
              \PY{c+c1}{\PYZsh{} npimg = img.numpy() }
              \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{npimg}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Misclassified with probabilities: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{,}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}c:.2f\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+si}{\PYZob{}end\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{c}\PY{o}{=}\PY{l+m+mi}{100}\PY{o}{*}\PY{n}{c}\PY{p}{,}\PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{i}\PY{o}{\PYZpc{}}\PY{k}{8}==0 else \PYZdq{}  \PYZdq{}) for i,(c,img) in enumerate(worst\PYZus{}false,1)]))
          \PY{n}{imshow}\PY{p}{(} \PY{n}{torchvision}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{p}{[}\PY{n}{img}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{c}\PY{p}{,}\PY{n}{img} \PY{o+ow}{in} \PY{n}{worst\PYZus{}false}\PY{p}{]}\PY{p}{)} \PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Correctly classified with probabilities: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{,}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}c:.0f\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+si}{\PYZob{}end\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{c}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{100}\PY{o}{*}\PY{n}{c}\PY{p}{,}\PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{i}\PY{o}{\PYZpc{}}\PY{k}{8}==0 else \PYZdq{}  \PYZdq{}) for i,(c,img) in enumerate(best\PYZus{}correct,1)]))
          \PY{n}{imshow}\PY{p}{(} \PY{n}{torchvision}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{p}{[}\PY{n}{img}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{c}\PY{p}{,}\PY{n}{img} \PY{o+ow}{in} \PY{n}{best\PYZus{}correct}\PY{p}{]}\PY{p}{)} \PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_75_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_75_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    If we, in fact, plot the histograms of the misclassifications and
correct classifications we can see that, on expectation, the model is
far more confident when it predicts correct labels than misclassified
cases. This is a good thing, it indicates that in a real-world
application where it is acceptable to label a picture "i don't know"
instead of just "cat" and "dog", one could set a threshold parameter
that controls the confidence level below which the nets says "i don't
know". The fact that the fraction of missclassified pictures increases
as the confidence level of the net decreases render this threshold
parameter meaningful. It could be useful in a context where we want to
avoid false-positive (i.e. wrong labels) and are fine with not labeling
all the pictures.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Histogram of images that were \PYZdl{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{bf}\PY{l+s+si}{\PYZob{}incorrectly\PYZcb{}}\PY{l+s+s2}{\PYZdl{} classified}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{FloatTensor}\PY{p}{(}\PY{n}{negativehisto}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{rwidth}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Histogram of images that were \PYZdl{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{bf}\PY{l+s+si}{\PYZob{}correctly\PYZcb{}}\PY{l+s+s2}{\PYZdl{} classified}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{FloatTensor}\PY{p}{(}\PY{n}{positivehisto}\PY{p}{)}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{rwidth}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_77_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Visualize feature maps}\label{visualize-feature-maps}

We further want to look into the kernels and feature maps and see if we
can see any meaningful output of them. Interpreting the feature maps in
the middle layers are no trivial task as the architecture is a
continuous combination of weights that are not neccessarily in the same
three layered regime of RGB channels. Zeiler et al., 2011 proposed a
novel way of projecting back feature maps back to the input space
through an architecture called deconvolutional networks (deconvnet).
Deconvnet were originally proposed for unsupervised learning, but here
they are only used to inspect the model's feature maps. They use
rectification, unpooling, and transpose of the kernels (used in
backpropagation of CNNs) corresponding to the original convnets (refer
to page 52 of Lecture\_3\_convnets.pdf).

Loading the pretrained model:

    \paragraph{This part was done to vizualise the feature maps of the
classifier5 model, so we load one we have stored. The previous cells
about missclassification statistics should be re-executed using the
loaded
model.}\label{this-part-was-done-to-vizualise-the-feature-maps-of-the-classifier5-model-so-we-load-one-we-have-stored.-the-previous-cells-about-missclassification-statistics-should-be-re-executed-using-the-loaded-model.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}139}]:} \PY{c+c1}{\PYZsh{} keep a reference to previous mynet}
          \PY{n}{mynet\PYZus{}old} \PY{o}{=} \PY{n}{mynet}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}140}]:} \PY{c+c1}{\PYZsh{} load a trained Classifier5}
          \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./save/export/dev1num3Classifier5\PYZus{}82.pth}\PY{l+s+s2}{\PYZdq{}}    \PY{c+c1}{\PYZsh{} Classifier5() with 82\PYZpc{} accuracy}
          \PY{n}{cudanet\PYZus{}tocpu} \PY{o}{=} \PY{n}{Classifier5}\PY{p}{(}\PY{p}{)} 
          \PY{n}{cudanet\PYZus{}tocpu}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
          \PY{n}{mynet} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(} \PY{n}{cudanet\PYZus{}tocpu} \PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    In the following we created a deconvnet of the chosen Classifier5
helping us examining the feature maps of our Classifier5 model. We
initialize its parameters with the pretrained convnet and created a user
interface to probe any arbitrary layer and unit. The only layers that we
skipped implementing was rectification as per convenience. The program
was inspired from the following link:
https://github.com/csgwon/pytorch-deconvnet/blob/master/models/vgg16\_deconv.py

The earlier featuremaps are showing how the model depicts more color
detection with a subtle features encoding, and as it progresses the
images despite being harder to interpret how the pieces com together,
but it looks as if it is forming a more defined form of cat and dog.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}141}]:} \PY{c+c1}{\PYZsh{} for vizualisation purpose}
          \PY{k}{class} \PY{n+nc}{Classifier5\PYZus{}extended}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
          \PY{l+s+sd}{    Classifier5 extension for vizualisation of feature maps}
          \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                  
                  \PY{n}{kernel\PYZus{}sz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
                  \PY{n}{pad} \PY{o}{=} \PY{n}{kernel\PYZus{}sz} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2} 
                  \PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{7}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0} 
                  
                  \PY{n+nb}{super}\PY{p}{(}\PY{n}{Classifier5\PYZus{}extended}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
                      \PY{c+c1}{\PYZsh{} Layer, input size = 64\PYZca{}2}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{return\PYZus{}indices}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
                      
                      \PY{c+c1}{\PYZsh{} Layer 2, input size = 32\PYZca{}2}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{return\PYZus{}indices}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
                      
                      \PY{c+c1}{\PYZsh{} Layer 3, input size = 16\PYZca{}2}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{return\PYZus{}indices}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
          
                      \PY{c+c1}{\PYZsh{} Layer 4, input size = 8\PYZca{}2}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
          
                      \PY{c+c1}{\PYZsh{} Layer 5}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{return\PYZus{}indices}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
                      
                      \PY{c+c1}{\PYZsh{} Layer 6}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{return\PYZus{}indices}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
                      
                      \PY{c+c1}{\PYZsh{} Layer 7}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{return\PYZus{}indices}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}           
                  \PY{p}{)}
                  \PY{c+c1}{\PYZsh{} }
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1b} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{512}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feature\PYZus{}outputs} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv}\PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{pool\PYZus{}indices} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{p}{)}
          
              \PY{k}{def} \PY{n+nf}{initialize\PYZus{}weights\PYZus{}from}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{classifier5}\PY{p}{)}\PY{p}{:}
                  \PY{c+c1}{\PYZsh{} initializing weights using ImageNet\PYZhy{}trained model from PyTorch}
                  \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{classifier5}\PY{o}{.}\PY{n}{conv}\PY{p}{)}\PY{p}{:}
                      \PY{k}{if} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{layer}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{)}\PY{p}{:}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1b}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{classifier5}\PY{o}{.}\PY{n}{fct1b}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1b}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data}   \PY{o}{=} \PY{n}{classifier5}\PY{o}{.}\PY{n}{fct1b}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data}
                  
              \PY{k}{def} \PY{n+nf}{forward\PYZus{}features}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
                  \PY{n}{output} \PY{o}{=} \PY{n}{x}
                  \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv}\PY{p}{)}\PY{p}{:}
                      \PY{k}{if} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{layer}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{MaxPool2d}\PY{p}{)}\PY{p}{:}
                          \PY{n}{output}\PY{p}{,} \PY{n}{indices} \PY{o}{=} \PY{n}{layer}\PY{p}{(}\PY{n}{output}\PY{p}{)}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feature\PYZus{}outputs}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{output}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{pool\PYZus{}indices}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{indices}
                      \PY{k}{else}\PY{p}{:}
                          \PY{n}{output} \PY{o}{=} \PY{n}{layer}\PY{p}{(}\PY{n}{output}\PY{p}{)}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feature\PYZus{}outputs}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{output}
                  \PY{k}{return} \PY{n}{output}
          
              \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
                  \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{forward\PYZus{}features}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                  \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                  \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fct1b}\PY{p}{(}\PY{n}{x}\PY{p}{)}
                  \PY{k}{return} \PY{n}{x}
          
          \PY{k}{class} \PY{n+nc}{declassifier}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Convnet Classifier\PYZdq{}\PYZdq{}\PYZdq{}}
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self} \PY{p}{)}\PY{p}{:}
                  
                  \PY{n}{kernel\PYZus{}sz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
                  \PY{n}{pad} \PY{o}{=} \PY{n}{kernel\PYZus{}sz} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2} 
                  \PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{7}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0} 
                  
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv2DeconvIdx} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{:}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{:}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{17}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{\PYZcb{}}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv2DeconvBiasIdx} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{:}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{:}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{14}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{17}\PY{p}{:}\PY{l+m+mi}{0}\PY{p}{\PYZcb{}}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{unpool2PoolIdx} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+m+mi}{11}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{:}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{:}\PY{l+m+mi}{13}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{18}\PY{p}{\PYZcb{}}
          
                  \PY{n+nb}{super}\PY{p}{(}\PY{n}{declassifier}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{)}\PY{p}{,}
                  \PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}first\PYZus{}layers} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{256}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{(}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{stride}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{in\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{out\PYZus{}channels}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=} \PY{p}{(}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{kernel\PYZus{}sz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{pad}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{)}\PY{p}{,}
                  \PY{p}{)}
                  \PY{c+c1}{\PYZsh{} }
          
              \PY{k}{def} \PY{n+nf}{initialize\PYZus{}weights\PYZus{}from}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{classifier5}\PY{p}{)}\PY{p}{:}
                  \PY{c+c1}{\PYZsh{} initializing weights using ImageNet\PYZhy{}trained model from PyTorch}
                  \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{classifier5}\PY{o}{.}\PY{n}{conv}\PY{p}{)}\PY{p}{:}
                      \PY{k}{if} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{layer}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{)}\PY{p}{:}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features}\PY{p}{[}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv2DeconvIdx}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data}
                          \PY{n}{biasIdx} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv2DeconvBiasIdx}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                          \PY{k}{if} \PY{n}{biasIdx} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
                              \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features}\PY{p}{[}\PY{n}{biasIdx}\PY{p}{]}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n}{layer}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data}
          
              \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{layer\PYZus{}number}\PY{p}{,} \PY{n}{map\PYZus{}number}\PY{p}{,} \PY{n}{pool\PYZus{}indices}\PY{p}{)}\PY{p}{:}
                  \PY{n}{start\PYZus{}idx} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv2DeconvIdx}\PY{p}{[}\PY{n}{layer\PYZus{}number}\PY{p}{]}
                  \PY{k}{if} \PY{o+ow}{not} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}first\PYZus{}layers}\PY{p}{[}\PY{n}{start\PYZus{}idx}\PY{p}{]}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{)}\PY{p}{:}
                      \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Layer }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{layer\PYZus{}number}\PY{p}{)}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ is not of type Conv2d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} set weight and bias}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}first\PYZus{}layers}\PY{p}{[}\PY{n}{start\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features}\PY{p}{[}\PY{n}{start\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{weight}\PY{p}{[}\PY{n}{map\PYZus{}number}\PY{p}{]}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}first\PYZus{}layers}\PY{p}{[}\PY{n}{start\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features}\PY{p}{[}\PY{n}{start\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data}        
                  \PY{c+c1}{\PYZsh{} first layer will be single channeled, since we\PYZsq{}re picking a particular filter}
                  \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}first\PYZus{}layers}\PY{p}{[}\PY{n}{start\PYZus{}idx}\PY{p}{]}\PY{p}{(}\PY{n}{x}\PY{p}{)}
          
                  \PY{c+c1}{\PYZsh{} transpose conv through the rest of the network}
                  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{start\PYZus{}idx}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                      \PY{k}{if} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{MaxUnpool2d}\PY{p}{)}\PY{p}{:}
                          \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{pool\PYZus{}indices}\PY{p}{[}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{unpool2PoolIdx}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{)}
                      \PY{k}{else}\PY{p}{:}
                          \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{deconv\PYZus{}features}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{(}\PY{n}{output}\PY{p}{)}
                  \PY{k}{return} \PY{n}{output}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}142}]:} \PY{k+kn}{from} \PY{n+nn}{math} \PY{k}{import} \PY{n}{sqrt}\PY{p}{,} \PY{n}{ceil}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          
          \PY{k}{def} \PY{n+nf}{visualize\PYZus{}grid}\PY{p}{(}\PY{n}{Xs}\PY{p}{,} \PY{n}{ubound}\PY{o}{=}\PY{l+m+mf}{255.0}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
          \PY{l+s+sd}{    Reshape a 4D tensor of image data to a grid for easy visualization.}
          \PY{l+s+sd}{    Inputs:}
          \PY{l+s+sd}{    \PYZhy{} Xs: Data of shape (N, H, W, C)}
          \PY{l+s+sd}{    \PYZhy{} ubound: Output grid will have values scaled to the range [0, ubound]}
          \PY{l+s+sd}{    \PYZhy{} padding: The number of blank pixels between elements of the grid}
          \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
              \PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{H}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{C}\PY{p}{)} \PY{o}{=} \PY{n}{Xs}\PY{o}{.}\PY{n}{shape}
              \PY{n}{grid\PYZus{}size} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{ceil}\PY{p}{(}\PY{n}{sqrt}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              \PY{n}{grid\PYZus{}height} \PY{o}{=} \PY{n}{H} \PY{o}{*} \PY{n}{grid\PYZus{}size} \PY{o}{+} \PY{n}{padding} \PY{o}{*} \PY{p}{(}\PY{n}{grid\PYZus{}size} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{grid\PYZus{}width} \PY{o}{=} \PY{n}{W} \PY{o}{*} \PY{n}{grid\PYZus{}size} \PY{o}{+} \PY{n}{padding} \PY{o}{*} \PY{p}{(}\PY{n}{grid\PYZus{}size} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{grid} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{grid\PYZus{}height}\PY{p}{,} \PY{n}{grid\PYZus{}width}\PY{p}{,} \PY{n}{C}\PY{p}{)}\PY{p}{)}
              \PY{n}{next\PYZus{}idx} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{n}{y0}\PY{p}{,} \PY{n}{y1} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{H}
              \PY{k}{for} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{grid\PYZus{}size}\PY{p}{)}\PY{p}{:}
                  \PY{n}{x0}\PY{p}{,} \PY{n}{x1} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{W}
                  \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{grid\PYZus{}size}\PY{p}{)}\PY{p}{:}
                      \PY{k}{if} \PY{n}{next\PYZus{}idx} \PY{o}{\PYZlt{}} \PY{n}{N}\PY{p}{:}
                          \PY{n}{img} \PY{o}{=} \PY{n}{Xs}\PY{p}{[}\PY{n}{next\PYZus{}idx}\PY{p}{]}
                          \PY{n}{low}\PY{p}{,} \PY{n}{high} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{img}\PY{p}{)}
                          \PY{n}{grid}\PY{p}{[}\PY{n}{y0}\PY{p}{:}\PY{n}{y1}\PY{p}{,} \PY{n}{x0}\PY{p}{:}\PY{n}{x1}\PY{p}{]} \PY{o}{=} \PY{n}{ubound} \PY{o}{*} \PY{p}{(}\PY{n}{img} \PY{o}{\PYZhy{}} \PY{n}{low}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{high} \PY{o}{\PYZhy{}} \PY{n}{low}\PY{p}{)}
                          \PY{c+c1}{\PYZsh{} grid[y0:y1, x0:x1] = Xs[next\PYZus{}idx]}
                          \PY{n}{next\PYZus{}idx} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                      \PY{n}{x0} \PY{o}{+}\PY{o}{=} \PY{n}{W} \PY{o}{+} \PY{n}{padding}
                      \PY{n}{x1} \PY{o}{+}\PY{o}{=} \PY{n}{W} \PY{o}{+} \PY{n}{padding}
                  \PY{n}{y0} \PY{o}{+}\PY{o}{=} \PY{n}{H} \PY{o}{+} \PY{n}{padding}
                  \PY{n}{y1} \PY{o}{+}\PY{o}{=} \PY{n}{H} \PY{o}{+} \PY{n}{padding}
              \PY{c+c1}{\PYZsh{} grid\PYZus{}max = np.max(grid)}
              \PY{c+c1}{\PYZsh{} grid\PYZus{}min = np.min(grid)}
              \PY{c+c1}{\PYZsh{} grid = ubound * (grid \PYZhy{} grid\PYZus{}min) / (grid\PYZus{}max \PYZhy{} grid\PYZus{}min)}
              \PY{k}{return} \PY{n}{grid}
          
          \PY{k}{def} \PY{n+nf}{vis\PYZus{}grid}\PY{p}{(}\PY{n}{Xs}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} visualize a grid of images \PYZdq{}\PYZdq{}\PYZdq{}}
              \PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{H}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{C}\PY{p}{)} \PY{o}{=} \PY{n}{Xs}\PY{o}{.}\PY{n}{shape}
              \PY{n}{A} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{ceil}\PY{p}{(}\PY{n}{sqrt}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              \PY{n}{G} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{A}\PY{o}{*}\PY{n}{H}\PY{o}{+}\PY{n}{A}\PY{p}{,} \PY{n}{A}\PY{o}{*}\PY{n}{W}\PY{o}{+}\PY{n}{A}\PY{p}{,} \PY{n}{C}\PY{p}{)}\PY{p}{,} \PY{n}{Xs}\PY{o}{.}\PY{n}{dtype}\PY{p}{)}
              \PY{n}{G} \PY{o}{*}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{Xs}\PY{p}{)}
              \PY{n}{n} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{k}{for} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{:}
                  \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{:}
                      \PY{k}{if} \PY{n}{n} \PY{o}{\PYZlt{}} \PY{n}{N}\PY{p}{:}
                          \PY{n}{G}\PY{p}{[}\PY{n}{y}\PY{o}{*}\PY{n}{H}\PY{o}{+}\PY{n}{y}\PY{p}{:}\PY{p}{(}\PY{n}{y}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{H}\PY{o}{+}\PY{n}{y}\PY{p}{,} \PY{n}{x}\PY{o}{*}\PY{n}{W}\PY{o}{+}\PY{n}{x}\PY{p}{:}\PY{p}{(}\PY{n}{x}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{W}\PY{o}{+}\PY{n}{x}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{Xs}\PY{p}{[}\PY{n}{n}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}
                          \PY{n}{n} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
              \PY{c+c1}{\PYZsh{} normalize to [0,1]}
              \PY{n}{maxg} \PY{o}{=} \PY{n}{G}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}
              \PY{n}{ming} \PY{o}{=} \PY{n}{G}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
              \PY{n}{G} \PY{o}{=} \PY{p}{(}\PY{n}{G} \PY{o}{\PYZhy{}} \PY{n}{ming}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{maxg}\PY{o}{\PYZhy{}}\PY{n}{ming}\PY{p}{)}
              \PY{k}{return} \PY{n}{G}
            
          \PY{k}{def} \PY{n+nf}{vis\PYZus{}nn}\PY{p}{(}\PY{n}{rows}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} visualize array of arrays of images \PYZdq{}\PYZdq{}\PYZdq{}}
              \PY{n}{N} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{rows}\PY{p}{)}
              \PY{n}{D} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{rows}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
              \PY{n}{H}\PY{p}{,}\PY{n}{W}\PY{p}{,}\PY{n}{C} \PY{o}{=} \PY{n}{rows}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}
              \PY{n}{Xs} \PY{o}{=} \PY{n}{rows}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
              \PY{n}{G} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{N}\PY{o}{*}\PY{n}{H}\PY{o}{+}\PY{n}{N}\PY{p}{,} \PY{n}{D}\PY{o}{*}\PY{n}{W}\PY{o}{+}\PY{n}{D}\PY{p}{,} \PY{n}{C}\PY{p}{)}\PY{p}{,} \PY{n}{Xs}\PY{o}{.}\PY{n}{dtype}\PY{p}{)}
              \PY{k}{for} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{:}
                  \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{D}\PY{p}{)}\PY{p}{:}
                      \PY{n}{G}\PY{p}{[}\PY{n}{y}\PY{o}{*}\PY{n}{H}\PY{o}{+}\PY{n}{y}\PY{p}{:}\PY{p}{(}\PY{n}{y}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{H}\PY{o}{+}\PY{n}{y}\PY{p}{,} \PY{n}{x}\PY{o}{*}\PY{n}{W}\PY{o}{+}\PY{n}{x}\PY{p}{:}\PY{p}{(}\PY{n}{x}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{n}{W}\PY{o}{+}\PY{n}{x}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{rows}\PY{p}{[}\PY{n}{y}\PY{p}{]}\PY{p}{[}\PY{n}{x}\PY{p}{]}
              \PY{c+c1}{\PYZsh{} normalize to [0,1]}
              \PY{n}{maxg} \PY{o}{=} \PY{n}{G}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}
              \PY{n}{ming} \PY{o}{=} \PY{n}{G}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
              \PY{n}{G} \PY{o}{=} \PY{p}{(}\PY{n}{G} \PY{o}{\PYZhy{}} \PY{n}{ming}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{maxg}\PY{o}{\PYZhy{}}\PY{n}{ming}\PY{p}{)}
              \PY{k}{return} \PY{n}{G}
\end{Verbatim}

    \paragraph{Display some feature maps}\label{display-some-feature-maps}

We retrieve a sample that the net correctly classified and with high
confidence and use it as input.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}170}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
          \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{Image}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{k+kn}{import} \PY{n+nn}{sys}
          
          \PY{k}{def} \PY{n+nf}{vis\PYZus{}layer}\PY{p}{(}\PY{n}{activ\PYZus{}map}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{clf}\PY{p}{(}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{121}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{activ\PYZus{}map}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{k}{def} \PY{n+nf}{decon\PYZus{}img}\PY{p}{(}\PY{n}{layer\PYZus{}output}\PY{p}{)}\PY{p}{:}
              \PY{n}{raw\PYZus{}img} \PY{o}{=} \PY{n}{layer\PYZus{}output}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
              \PY{n}{img} \PY{o}{=} \PY{p}{(}\PY{n}{raw\PYZus{}img}\PY{o}{\PYZhy{}}\PY{n}{raw\PYZus{}img}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{raw\PYZus{}img}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{raw\PYZus{}img}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{255}
              \PY{n}{img} \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{uint8}\PY{p}{)}
              \PY{k}{return} \PY{n}{img}
          
          \PY{k}{if} \PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}\PYZus{}main\PYZus{}\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
              \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{sys}\PY{o}{.}\PY{n}{argv}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{l+m+mi}{2}\PY{p}{:}
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Usage: }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n}{sys}\PY{o}{.}\PY{n}{argv}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ img\PYZus{}file}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{n}{sys}\PY{o}{.}\PY{n}{exit}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
          
              \PY{n}{img\PYZus{}filename} \PY{o}{=} \PY{n}{sys}\PY{o}{.}\PY{n}{argv}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
          
              \PY{n}{n\PYZus{}classes} \PY{o}{=} \PY{l+m+mi}{1000} \PY{c+c1}{\PYZsh{} using ImageNet pretrained weights}
          
              \PY{c+c1}{\PYZsh{}vgg16\PYZus{}c = VGG16\PYZus{}conv(n\PYZus{}classes)}
              \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{mynet}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
              \PY{n}{mynet\PYZus{}extended} \PY{o}{=} \PY{n}{Classifier5\PYZus{}extended}\PY{p}{(}\PY{p}{)}
              \PY{n}{mynet\PYZus{}extended}\PY{o}{.}\PY{n}{initialize\PYZus{}weights\PYZus{}from}\PY{p}{(}\PY{n}{mynet}\PY{p}{)}
              \PY{n}{cudanet\PYZus{}d} \PY{o}{=} \PY{n}{declassifier}\PY{p}{(}\PY{p}{)}
              \PY{n}{cudanet\PYZus{}d}\PY{o}{.}\PY{n}{initialize\PYZus{}weights\PYZus{}from}\PY{p}{(}\PY{n}{mynet}\PY{p}{)}
              
              \PY{n}{conv\PYZus{}layer\PYZus{}indices} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{cudanet\PYZus{}d}\PY{o}{.}\PY{n}{conv2DeconvIdx}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{}img = np.asarray(Image.open(img\PYZus{}filename).resize((224,224)))}
              \PY{n}{img\PYZus{}var} \PY{o}{=} \PY{n}{best\PYZus{}correct}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
              \PY{c+c1}{\PYZsh{}img\PYZus{}var = torch.autograd.Variable(torch.FloatTensor(img.transpose(2,0,1)[np.newaxis,:,:,:].astype(float)))}
          
              \PY{n}{conv\PYZus{}out} \PY{o}{=} \PY{n}{mynet\PYZus{}extended}\PY{p}{(}\PY{n}{img\PYZus{}var}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Classifier5 model:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{n}{mynet}\PY{p}{)}
          
              \PY{n}{plt}\PY{o}{.}\PY{n}{ion}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} remove blocking}
              \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
          
              \PY{n}{layer} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(} \PY{n+nb}{input}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Layer to view (0\PYZhy{}17, \PYZhy{}1 to exit): }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{p}{)}
          
              \PY{n}{activ\PYZus{}map} \PY{o}{=} \PY{n}{mynet\PYZus{}extended}\PY{o}{.}\PY{n}{feature\PYZus{}outputs}\PY{p}{[}\PY{n}{layer}\PY{p}{]}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}
              \PY{n}{activ\PYZus{}map} \PY{o}{=} \PY{n}{activ\PYZus{}map}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
              \PY{n}{activ\PYZus{}map\PYZus{}grid} \PY{o}{=} \PY{n}{vis\PYZus{}grid}\PY{p}{(}\PY{n}{activ\PYZus{}map}\PY{p}{)}
              \PY{n}{vis\PYZus{}layer}\PY{p}{(}\PY{n}{activ\PYZus{}map\PYZus{}grid}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} only transpose convolve from Conv2d or ReLU layers}
              \PY{n}{conv\PYZus{}layer} \PY{o}{=} \PY{n}{layer}
              \PY{k}{if} \PY{n}{conv\PYZus{}layer} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{conv\PYZus{}layer\PYZus{}indices}\PY{p}{:}
                  \PY{n}{conv\PYZus{}layer} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{l+m+mi}{1}
                  \PY{k}{if} \PY{n}{conv\PYZus{}layer} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{conv\PYZus{}layer\PYZus{}indices}\PY{p}{:}
                      \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Invalid Layer Number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
              \PY{n}{n\PYZus{}maps} \PY{o}{=} \PY{n}{activ\PYZus{}map}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
          
              \PY{n}{map\PYZus{}idx} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(} \PY{n+nb}{input}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Take a map to view (0\PYZhy{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{): }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{activ\PYZus{}map}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{p}{)}
          
              \PY{n}{decon} \PY{o}{=} \PY{n}{cudanet\PYZus{}d}\PY{p}{(}\PY{n}{mynet\PYZus{}extended}\PY{o}{.}\PY{n}{feature\PYZus{}outputs}\PY{p}{[}\PY{n}{layer}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{map\PYZus{}idx}\PY{p}{]}\PY{p}{[}\PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{conv\PYZus{}layer}\PY{p}{,} \PY{n}{map\PYZus{}idx}\PY{p}{,} \PY{n}{mynet\PYZus{}extended}\PY{o}{.}\PY{n}{pool\PYZus{}indices}\PY{p}{)}
              \PY{n}{img} \PY{o}{=} \PY{n}{decon\PYZus{}img}\PY{p}{(}\PY{n}{decon}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Classifier5 model:
Classifier5(
  (conv): Sequential(
    (0): Conv2d(3, 16, kernel\_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU()
    (2): MaxPool2d(kernel\_size=(2, 2), stride=2, padding=0, dilation=1, ceil\_mode=False)
    (3): Conv2d(16, 32, kernel\_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU()
    (5): MaxPool2d(kernel\_size=(2, 2), stride=2, padding=0, dilation=1, ceil\_mode=False)
    (6): Conv2d(32, 64, kernel\_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (7): ReLU()
    (8): MaxPool2d(kernel\_size=(2, 2), stride=2, padding=0, dilation=1, ceil\_mode=False)
    (9): Conv2d(64, 128, kernel\_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): ReLU()
    (11): Conv2d(128, 256, kernel\_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): ReLU()
    (13): MaxPool2d(kernel\_size=(2, 2), stride=2, padding=0, dilation=1, ceil\_mode=False)
    (14): Conv2d(256, 256, kernel\_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU()
    (16): MaxPool2d(kernel\_size=(2, 2), stride=2, padding=0, dilation=1, ceil\_mode=False)
    (17): Conv2d(256, 512, kernel\_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): MaxPool2d(kernel\_size=(2, 2), stride=2, padding=0, dilation=1, ceil\_mode=False)
  )
  (fct1b): Linear(in\_features=512, out\_features=2, bias=True)
)
Layer to view (0-17, -1 to exit): 0
Take a map to view (0-15): 1

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_86_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{vizualize kernels}\label{vizualize-kernels}

We display, a subset of kernels for each convolution layer. The kernel
are small and they don't look like edge detector. There is not much else
to be said.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}164}]:} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{p}{(}\PY{n}{name}\PY{p}{,} \PY{n}{kernels}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{mynet\PYZus{}extended}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{p}{:}
              \PY{c+c1}{\PYZsh{} ou \PYZdq{}conv\PYZdq{}}
              \PY{k}{if} \PY{o+ow}{not} \PY{p}{(}\PY{p}{(}\PY{n}{name}\PY{o}{.}\PY{n}{startswith}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{features}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{o+ow}{or} \PY{n}{name}\PY{o}{.}\PY{n}{startswith}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{conv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{o+ow}{and} \PY{n}{name}\PY{o}{.}\PY{n}{endswith}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{p}{)}\PY{p}{:}
                  \PY{k}{continue} 
              \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{0} \PY{p}{:} \PY{c+c1}{\PYZsh{} 3 input channels can be displayed in color}
                   \PY{n}{kernels} \PY{o}{=} \PY{n}{kernels}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}
              \PY{k}{else} \PY{p}{:} \PY{c+c1}{\PYZsh{} more than 3 input channels are displayed in greyscale}
                  \PY{n}{kernels} \PY{o}{=} \PY{n}{kernels}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{kernels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{kernels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             
              \PY{n}{kernels} \PY{o}{=} \PY{n}{kernels} \PY{o}{\PYZhy{}} \PY{n}{kernels}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
              \PY{n}{kernels} \PY{o}{=} \PY{l+m+mi}{255} \PY{o}{*} \PY{n}{kernels} \PY{o}{/} \PY{n}{kernels}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} 
              \PY{n}{size} \PY{o}{=} \PY{n+nb}{min}\PY{p}{(}\PY{n}{kernels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{)}
              \PY{c+c1}{\PYZsh{} print( kernels.max() , kernels.min() )}
              \PY{c+c1}{\PYZsh{} print(kernels.size())}
              \PY{n}{imshow}\PY{p}{(}\PY{n}{torchvision}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{kernels}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{n}{size}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_88_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_88_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_88_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_88_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_88_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_88_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_88_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Submit}\label{submit}

This part of the notebook is used for submission.

    \begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  We define a dataset for the test samples\\
\item
  We load the test dataset, label all picture and produce a .csv file
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}137}]:} \PY{k}{class} \PY{n+nc}{nonlabeledDataSet}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{Dataset}\PY{p}{)}\PY{p}{:}
          
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self} \PY{p}{,} \PY{n}{nb\PYZus{}of\PYZus{}sample}\PY{p}{,} \PY{n}{root\PYZus{}dir} \PY{p}{)}\PY{p}{:}
                  \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
          \PY{l+s+sd}{        Args:}
          \PY{l+s+sd}{            label is either \PYZdq{}Cat\PYZdq{} or \PYZdq{}Dog\PYZdq{}}
          \PY{l+s+sd}{            load in the dataset picture no. idx\PYZus{}min to idx\PYZus{}max included}
          \PY{l+s+sd}{            root\PYZus{}dir(string): directory with all images with the same label}
          \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                  \PY{c+c1}{\PYZsh{} super(labeledDataSet, self).\PYZus{}\PYZus{}init\PYZus{}\PYZus{}()  }
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root\PYZus{}dir} \PY{o}{=} \PY{n}{root\PYZus{}dir}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nb\PYZus{}of\PYZus{}sample} \PY{o}{=} \PY{n}{nb\PYZus{}of\PYZus{}sample}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{load\PYZus{}data}\PY{p}{(}\PY{p}{)}
              
              \PY{k}{def} \PY{n+nf}{load\PYZus{}data}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)} \PY{p}{:}
                  \PY{n}{size} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
                  
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{data\PYZus{}tensor}   \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{size}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{64}\PY{p}{,}\PY{l+m+mi}{64}\PY{p}{,}\PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{float}\PY{p}{)}
                  
                  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nb\PYZus{}of\PYZus{}sample}\PY{p}{)} \PY{p}{:}
                      \PY{n}{j} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}
                      \PY{n}{img\PYZus{}path} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{root\PYZus{}dir} \PY{o}{+}  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}index\PYZcb{}}\PY{l+s+s2}{.jpg}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{index}\PY{o}{=}\PY{n}{j}\PY{p}{)}
                      \PY{n}{img} \PY{o}{=} \PY{n}{Image}\PY{o}{.}\PY{n}{open}\PY{p}{(} \PY{n}{img\PYZus{}path} \PY{p}{)}\PY{o}{.}\PY{n}{convert}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RGB}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      \PY{n}{image} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(} \PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(} \PY{n}{img} \PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)} \PY{p}{)} \PY{p}{)}
                      \PY{n}{image} \PY{o}{=} \PY{n}{image}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{data\PYZus{}tensor}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{image}
                  
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}len\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nb\PYZus{}of\PYZus{}sample}
          
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}getitem\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{idx}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{data\PYZus{}tensor}\PY{p}{[}\PY{n}{idx}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}186}]:} \PY{c+c1}{\PYZsh{} Test the performance of mynet before using it}
          \PY{n}{criterion} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}
          \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{64}
          \PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
          \PY{n}{valid\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
          \PY{n}{a}\PY{p}{,}\PY{n}{b} \PY{o}{=} \PY{n}{measure\PYZus{}single\PYZus{}accuracy\PYZus{}and\PYZus{}loss}\PY{p}{(} \PY{n}{mynet} \PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{,} \PY{n}{criterion} \PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{a}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{loss : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{b}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          \PY{n}{a}\PY{p}{,}\PY{n}{b} \PY{o}{=} \PY{n}{measure\PYZus{}single\PYZus{}accuracy\PYZus{}and\PYZus{}loss}\PY{p}{(} \PY{n}{mynet} \PY{p}{,} \PY{n}{valid\PYZus{}loader}\PY{p}{,} \PY{n}{criterion} \PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{a}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{loss : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{b}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
97.13873291015625 loss :  0.00036534047103486955
86.59329986572266 loss :  0.0013806667411699891

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}187}]:} \PY{n}{testset\PYZus{}dir}        \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./data\PYZus{}catdogs/testset/test/}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{batch\PYZus{}size}         \PY{o}{=} \PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{64}
          \PY{n}{total\PYZus{}nb\PYZus{}of\PYZus{}sample} \PY{o}{=} \PY{l+m+mi}{4999} \PY{c+c1}{\PYZsh{} total number of total unlabelled test samples}
          \PY{n}{test\PYZus{}dataset} \PY{o}{=} \PY{n}{nonlabeledDataSet}\PY{p}{(}\PY{n}{total\PYZus{}nb\PYZus{}of\PYZus{}sample}\PY{p}{,}\PY{n}{testset\PYZus{}dir}\PY{p}{)}
          \PY{n}{test\PYZus{}loader}  \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
          
          \PY{k}{def} \PY{n+nf}{class\PYZus{}from\PYZus{}index}\PY{p}{(}\PY{n}{ind}\PY{p}{)}\PY{p}{:}
              \PY{k}{return} \PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{o}{.}\PY{n}{classes}\PY{p}{[}\PY{n}{ind}\PY{p}{]}
          
          \PY{n}{remember\PYZus{}prediction} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{batch\PYZus{}size}\PY{p}{)}
          \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{submission4.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{submission}\PY{p}{:}
              \PY{n}{submission}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id,label}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{1}
              \PY{k}{for} \PY{n}{query} \PY{o+ow}{in} \PY{n}{test\PYZus{}loader}\PY{p}{:}
                  \PY{n}{img} \PY{o}{=} \PY{n}{query}
                  \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                      \PY{n}{img}          \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                      \PY{n}{outputs}      \PY{o}{=} \PY{n}{mynet}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
                      \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                      \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{1} \PY{p}{:}
                          \PY{n}{remember\PYZus{}prediction} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{predicted}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
                      \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{predicted}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{p}{:}
                          \PY{n}{idx}    \PY{o}{=} \PY{n}{predicted}\PY{p}{[}\PY{n}{j}\PY{p}{]}
                          \PY{n}{label}  \PY{o}{=} \PY{n}{class\PYZus{}from\PYZus{}index}\PY{p}{(}\PY{n}{idx}\PY{p}{)}
                          \PY{n}{submission}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{,}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{n}{label}\PY{p}{)} \PY{p}{)}
                          \PY{n}{i} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}
\end{Verbatim}

    \paragraph{For sanity}\label{for-sanity}

We have manually labeled the first 100 pictures to be certain that the
data loader used was not feeding the pictures in the wrong order.
Previously, we had this issue.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}188}]:} \PY{n}{good\PYZus{}test\PYZus{}answers} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,}\PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{long}\PY{p}{)}
          \PY{n}{dog\PYZus{}idx} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{14}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{17}\PY{p}{,}\PY{l+m+mi}{19}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{22}\PY{p}{,}\PY{l+m+mi}{24}\PY{p}{,}\PY{l+m+mi}{26}\PY{p}{,}\PY{l+m+mi}{29}\PY{p}{,}\PY{l+m+mi}{31}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{39}\PY{p}{,}\PY{l+m+mi}{41}\PY{p}{,}\PY{l+m+mi}{43}\PY{p}{,}\PY{l+m+mi}{45}\PY{p}{,}\PY{l+m+mi}{53}\PY{p}{,}\PY{l+m+mi}{58}\PY{p}{,}\PY{l+m+mi}{61}\PY{p}{,}
                     \PY{l+m+mi}{63}\PY{p}{,}\PY{l+m+mi}{69}\PY{p}{,}\PY{l+m+mi}{70}\PY{p}{,}\PY{l+m+mi}{71}\PY{p}{,}\PY{l+m+mi}{74}\PY{p}{,}\PY{l+m+mi}{75}\PY{p}{,}\PY{l+m+mi}{76}\PY{p}{,}\PY{l+m+mi}{77}\PY{p}{,}\PY{l+m+mi}{82}\PY{p}{,}\PY{l+m+mi}{83}\PY{p}{,}\PY{l+m+mi}{84}\PY{p}{,}\PY{l+m+mi}{86}\PY{p}{,}\PY{l+m+mi}{87}\PY{p}{,}\PY{l+m+mi}{89}\PY{p}{,}\PY{l+m+mi}{93}\PY{p}{,}\PY{l+m+mi}{94}\PY{p}{,}\PY{l+m+mi}{97}\PY{p}{,}\PY{l+m+mi}{98}\PY{p}{,}\PY{l+m+mi}{99}\PY{p}{]}
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{dog\PYZus{}idx} \PY{p}{:}
              \PY{n}{good\PYZus{}test\PYZus{}answers}\PY{p}{[}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
              
          \PY{n}{count} \PY{o}{=} \PY{l+m+mi}{0}
          \PY{n}{prediction} \PY{o}{=} \PY{n}{remember\PYZus{}prediction}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{:}
              \PY{k}{if} \PY{n}{prediction}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{n}{good\PYZus{}test\PYZus{}answers}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{p}{:}
                  \PY{n}{count} \PY{o}{=} \PY{n}{count} \PY{o}{+} \PY{l+m+mi}{1}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of good answer for first 100 samples : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{count}\PY{p}{)} 
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of good answer for first 100 samples :  86

    \end{Verbatim}

    \paragraph{For sanity also}\label{for-sanity-also}

Manually check that the loader loads the picture in the good order.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}118}]:} \PY{n}{test\PYZus{}pict\PYZus{}loader}  \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{test\PYZus{}dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{,}\PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
          \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{img} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{test\PYZus{}pict\PYZus{}loader}\PY{p}{)}\PY{p}{:}
              \PY{k}{if} \PY{n}{i} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1} \PY{p}{:}
                  \PY{k}{break}
              \PY{n}{imshow}\PY{p}{(}\PY{n}{torchvision}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_97_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{dev1num3_files/dev1num3_97_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Save and load models}\label{save-and-load-models}

    \paragraph{Load}\label{load}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{c+c1}{\PYZsh{} On github}
         \PY{n}{loading\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./save/export/dev1num3Classifier5\PYZus{}82.pth}\PY{l+s+s2}{\PYZdq{}}    \PY{c+c1}{\PYZsh{} Classifier5() with 82\PYZpc{} accuracy}
         \PY{c+c1}{\PYZsh{} loading\PYZus{}path = \PYZdq{}./save/export/dev1num3VGGClassifier5\PYZus{}85.pth\PYZdq{} \PYZsh{} VGGClassifier() with 85\PYZpc{} accuracy}
         \PY{c+c1}{\PYZsh{} loading\PYZus{}path = \PYZdq{}./save/export/dev1num3VGGClassifier\PYZus{}86.pth\PYZdq{}  \PYZsh{} VGGClassifier() with 86\PYZpc{} accuracy}
         
         \PY{c+c1}{\PYZsh{} Locally only}
         \PY{c+c1}{\PYZsh{} loading\PYZus{}path = \PYZdq{}./save/classifier1\PYZus{}201to500/dev1num3model\PYZus{}for\PYZus{}epoch300.pth\PYZdq{} \PYZsh{} Classifier1()}
         \PY{c+c1}{\PYZsh{} loading\PYZus{}path = \PYZdq{}./save/underfit201to300/dev1num3model\PYZus{}for\PYZus{}epoch100.pth\PYZdq{} \PYZsh{} Classifier5d()}
         \PY{c+c1}{\PYZsh{} loading\PYZus{}path = \PYZdq{}./save/classifier5wsm\PYZus{}nocrop\PYZus{}51to100/dev1num3model\PYZus{}for\PYZus{}epoch50.pth\PYZdq{} \PYZsh{} Classifier5()}
         
         \PY{c+c1}{\PYZsh{} cudanet\PYZus{}tocpu = VGGClassifier() }
         \PY{n}{cudanet\PYZus{}tocpu} \PY{o}{=} \PY{n}{Classifier5}\PY{p}{(}\PY{p}{)} 
         \PY{n}{cudanet\PYZus{}tocpu}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{loading\PYZus{}path}\PY{p}{)}\PY{p}{)}
         \PY{n}{mynet} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(} \PY{n}{cudanet\PYZus{}tocpu} \PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\end{Verbatim}

    \paragraph{Save}\label{save}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}189}]:} \PY{c+c1}{\PYZsh{} save current state only}
          \PY{c+c1}{\PYZsh{} saving\PYZus{}path = \PYZdq{}./save/export/dev1num3model.pth\PYZdq{}}
          \PY{n}{saving\PYZus{}path} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./save/export/}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{saving\PYZus{}name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dev1num3BLABLA.pth}\PY{l+s+s2}{\PYZdq{}}
          \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{mynet}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
          \PY{n}{state\PYZus{}dict\PYZus{}to\PYZus{}disk} \PY{o}{=} \PY{n}{mynet}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}
          \PY{n}{torch}\PY{o}{.}\PY{n}{save}\PY{p}{(} \PY{n}{state\PYZus{}dict\PYZus{}to\PYZus{}disk} \PY{p}{,} \PY{n}{saving\PYZus{}path} \PY{p}{)}
          \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{mynet}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\end{Verbatim}

    \paragraph{Measure accuracy and average loss on training and validation
dataset}\label{measure-accuracy-and-average-loss-on-training-and-validation-dataset}

This is usefull is you load a previously saved model and want to measure
its performance.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} mynet  = Classifier5()}
        \PY{c+c1}{\PYZsh{} mynet.load\PYZus{}state\PYZus{}dict(net1\PYZus{}state\PYZus{}dict\PYZus{}list[30])}
        \PY{n}{mynet} \PY{o}{=} \PY{n}{cudanet2}
        \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{mynet}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        
        \PY{n}{criterion\PYZus{}sum} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{n}{reduction}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{64}
        \PY{c+c1}{\PYZsh{} train\PYZus{}loader = DataLoader(train\PYZus{}dataset\PYZus{}norm, batch\PYZus{}size=batch\PYZus{}size,sampler=train\PYZus{}sampler, num\PYZus{}workers=num\PYZus{}workers)}
        \PY{n}{valid\PYZus{}loader} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
        \PY{n}{a}\PY{p}{,}\PY{n}{b} \PY{o}{=} \PY{n}{measure\PYZus{}single\PYZus{}accuracy\PYZus{}and\PYZus{}loss}\PY{p}{(} \PY{n}{mynet} \PY{p}{,} \PY{n}{train\PYZus{}loader}\PY{p}{,} \PY{n}{criterion\PYZus{}sum} \PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training dataset}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{a}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{loss : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{b}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{a}\PY{p}{,}\PY{n}{b} \PY{o}{=} \PY{n}{measure\PYZus{}single\PYZus{}accuracy\PYZus{}and\PYZus{}loss}\PY{p}{(} \PY{n}{mynet} \PY{p}{,} \PY{n}{valid\PYZus{}loader}\PY{p}{,} \PY{n}{criterion\PYZus{}sum} \PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation dataset}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{a}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{loss : }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{b}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \section{Other comments}\label{other-comments}

    \subsubsection{Use majority vote}\label{use-majority-vote}

Use and odd number of net to find what they each think of a picture and
take the majority vote among them.\\
This is usefull to see if multiple nets "are independant sources of
information" or if "they all learnt the same things".\\
We used this method together with 3 different models achieving at least
80\% accuracy on the validation dataset : - Classifier5 (trained using
medium-low data augmentation) - Classifier7 (trained using medium data
augmentation) - VGGClassifier (trained using medium-low data
augmentation) We found that this method could be used to improve the
performance of the best of the 3 models by about 1\%.

If we take the majority vote for yes-no questions using 3 independants
voters that vote randomly with 80\% accuracy each. The probability of
the outcome of the vote to be right is :\\
- Prob(3 are right) + 3 Prob(2 are right)Prob(1 is wrong) = Prob(1 is
right)\^{}3 + 3 Prob(1 is right)\^{}2 Prob(1 is wrong) = (0.8)\^{}3 +
3(0.8)\^{}2(0.2) = 0.896

This indicates that the three models we have tested cannot possibly be
considered as independant. Even with different architectures, the 3
models have learnt very similar things about the classification task.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}168}]:} \PY{n}{cudanet1} \PY{o}{=} \PY{n}{Classifier5}\PY{p}{(}\PY{p}{)}
          \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{cudanet1}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
          \PY{n}{cudanet2} \PY{o}{=} \PY{n}{Classifier5}\PY{p}{(}\PY{p}{)}
          \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{cudanet2}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
          \PY{n}{cudanet3} \PY{o}{=} \PY{n}{Classifier5}\PY{p}{(}\PY{p}{)}
          \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{cudanet3}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}169}]:} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4}\PY{o}{*}\PY{l+m+mi}{64}
          
          \PY{c+c1}{\PYZsh{} with or without data augmentation}
          \PY{n}{validation\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{valid\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} validation\PYZus{}loader = torch.utils.data.DataLoader(train\PYZus{}dataset\PYZus{}augm, batch\PYZus{}size=batch\PYZus{}size,sampler=valid\PYZus{}sampler, num\PYZus{}workers=num\PYZus{}workers)}
          
          \PY{c+c1}{\PYZsh{} If set to true, the answers of the }
          \PY{n}{majority\PYZus{}by\PYZus{}confidence} \PY{o}{=} \PY{k+kc}{False}
          
          \PY{n}{correct} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
          \PY{n}{total} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             
          \PY{n}{correct}\PY{p}{,} \PY{n}{total} \PY{o}{=} \PY{n}{correct}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)} \PY{p}{,} \PY{n}{total}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)} 
          \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
              \PY{k}{for} \PY{n}{data} \PY{o+ow}{in} \PY{n}{validation\PYZus{}loader}\PY{p}{:}
                  \PY{n}{images}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{data}
                  \PY{c+c1}{\PYZsh{} if using BCE}
                  \PY{c+c1}{\PYZsh{} labels = labels.float()}
                  \PY{n}{images}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{images}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{,} \PY{n}{labels}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
                  
                  \PY{k}{if} \PY{n}{majority\PYZus{}by\PYZus{}confidence} \PY{p}{:}
                      \PY{n}{outputs} \PY{o}{=}  \PY{n}{torch}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{cudanet1}\PY{p}{(}\PY{n}{images}\PY{p}{)}\PY{p}{,}\PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{cudanet2}\PY{p}{(}\PY{n}{images}\PY{p}{)}\PY{p}{,}\PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{n}{torch}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{cudanet3}\PY{p}{(}\PY{n}{images}\PY{p}{)}\PY{p}{,}\PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} 
                      \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                  \PY{k}{else} \PY{p}{:}
                      \PY{n}{outputs} \PY{o}{=} \PY{n}{cudanet1}\PY{p}{(}\PY{n}{images}\PY{p}{)}
                      \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted1} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                      \PY{n}{outputs} \PY{o}{=} \PY{n}{cudanet2}\PY{p}{(}\PY{n}{images}\PY{p}{)}
                      \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted2} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                      \PY{n}{outputs} \PY{o}{=} \PY{n}{cudanet3}\PY{p}{(}\PY{n}{images}\PY{p}{)}
                      \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted3} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                      \PY{n}{predicted} \PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{(}\PY{n}{predicted1}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{predicted2}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{predicted3}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} print(predicted1.shape)}
                  \PY{c+c1}{\PYZsh{} print(predicted.shape)}
                  \PY{n}{total} \PY{o}{+}\PY{o}{=} \PY{n}{labels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
                  \PY{n}{correct} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{predicted} \PY{o}{==} \PY{n}{labels}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
          
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy of the network on the}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{n}{total}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)} \PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test images: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} 
                    \PY{o}{\PYZpc{}} \PY{p}{(} \PY{p}{(}\PY{l+m+mi}{100} \PY{o}{*} \PY{n}{correct}\PY{o}{.}\PY{n}{double}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{total}\PY{o}{.}\PY{n}{double}\PY{p}{(}\PY{p}{)}  \PY{p}{)}
               \PY{p}{)} 
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy of the network on the 1999 test images: 48.42 \%

    \end{Verbatim}

    \subsubsection{Find a good
initialization}\label{find-a-good-initialization}

The following code is usefull to find a good initialization if finding
one appears to be hard work. We used this code to make sure that certain
configurations did not work at all.\\
It works as follow :\\
Try different random init, train them for 3 epoch, repeat until you find
one than has learnt something or the number of tries reach a certain
threshold.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} del cudanet }
        \PY{n}{nb\PYZus{}epoch}  \PY{o}{=} \PY{l+m+mi}{1}
        \PY{n}{nb\PYZus{}try}    \PY{o}{=} \PY{l+m+mi}{10}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{l+m+mi}{16}
        \PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{train\PYZus{}dataset\PYZus{}norm}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}\PY{n}{sampler}\PY{o}{=}\PY{n}{train\PYZus{}sampler}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{n}{num\PYZus{}workers}\PY{p}{)}
        
        \PY{n}{state\PYZus{}dict\PYZus{}list} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{start} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{Event}\PY{p}{(}\PY{n}{enable\PYZus{}timing}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{end}   \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{Event}\PY{p}{(}\PY{n}{enable\PYZus{}timing}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{start}\PY{o}{.}\PY{n}{record}\PY{p}{(}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{trial} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{nb\PYZus{}try}\PY{p}{)} \PY{p}{:}
            \PY{n}{cudanet} \PY{o}{=} \PY{n}{Classifier5}\PY{p}{(}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} cudanet = Classifier1b(sigmoid=True)}
            \PY{n}{cudanet}\PY{o}{.}\PY{n}{apply}\PY{p}{(} \PY{n}{glorot\PYZus{}init} \PY{p}{)}
            \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{cudanet}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
            \PY{n}{criterion} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} optimizer = optim.SGD(cudanet.parameters(), lr=0.00025, momentum=0, weight\PYZus{}decay=0)}
            \PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{cudanet}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{,} \PY{n}{momentum}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{weight\PYZus{}decay}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{n}{correct} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{n}{total} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(} \PY{n}{nb\PYZus{}epoch} \PY{p}{)}\PY{p}{:}  \PY{c+c1}{\PYZsh{} loop over the dataset multiple times }
        
                \PY{n}{running\PYZus{}loss} \PY{o}{=} \PY{l+m+mf}{0.0}
                \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{inputs}\PY{p}{,} \PY{n}{labels}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
                    \PY{c+c1}{\PYZsh{} if using BCE :}
                    \PY{c+c1}{\PYZsh{} labels = labels.float() }
                    \PY{n}{inputs}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{inputs}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{,} \PY{n}{labels}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        
                    \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
                    \PY{n}{outputs} \PY{o}{=} \PY{n}{cudanet}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
                    \PY{n}{loss} \PY{o}{=} \PY{n}{criterion}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
                    \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
                    \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
        
                    \PY{c+c1}{\PYZsh{} print statistics}
                    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)} \PY{p}{:}
                        \PY{n}{running\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
                        \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                        \PY{n}{total} \PY{o}{+}\PY{o}{=} \PY{n}{labels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
                        \PY{n}{correct} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{predicted} \PY{o}{==} \PY{n}{labels}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
                \PY{k}{else} \PY{p}{:} \PY{c+c1}{\PYZsh{} print every epoch}
                    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trial }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ , epoch = }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{, loss = }\PY{l+s+si}{\PYZpc{}.8f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{trial} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{epoch} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{running\PYZus{}loss} \PY{o}{/} \PY{n}{training\PYZus{}dataset\PYZus{}size}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} nb of sample per mini\PYZhy{}batch}
                    \PY{n}{running\PYZus{}loss} \PY{o}{=} \PY{l+m+mf}{0.0}
                    \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
                    \PY{n}{tmp\PYZus{}state\PYZus{}dict} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
                    \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{v} \PY{o+ow}{in} \PY{n}{cudanet}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                        \PY{n}{tmp\PYZus{}state\PYZus{}dict}\PY{p}{[}\PY{n}{k}\PY{p}{]} \PY{o}{=} \PY{n}{v}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
                    \PY{n}{state\PYZus{}dict\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(} \PY{n}{tmp\PYZus{}state\PYZus{}dict} \PY{p}{)}
                    \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
            \PY{n}{accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{correct}\PY{o}{.}\PY{n}{double}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n}{total}\PY{o}{.}\PY{n}{double}\PY{p}{(}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy for trial }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{ : }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{trial}\PY{o}{+}\PY{l+m+mi}{1} \PY{p}{,} \PY{n}{accuracy}\PY{p}{)} \PY{p}{)}
            \PY{k}{if} \PY{n}{accuracy} \PY{o}{\PYZgt{}} \PY{l+m+mi}{53} \PY{p}{:}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Successful search}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{k}{break}
            \PY{k}{del} \PY{n}{cudanet}
        \PY{k}{else} \PY{p}{:} 
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unsuccessful search}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
        \PY{n}{end}\PY{o}{.}\PY{n}{record}\PY{p}{(}\PY{p}{)}
        \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{synchronize}\PY{p}{(}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{time required = }\PY{l+s+s2}{\PYZdq{}} \PY{p}{,} \PY{n}{start}\PY{o}{.}\PY{n}{elapsed\PYZus{}time}\PY{p}{(}\PY{n}{end}\PY{p}{)}\PY{o}{*}\PY{l+m+mf}{0.001} \PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ s }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{k}{if} \PY{n}{want\PYZus{}lound\PYZus{}warning} \PY{p}{:}
            \PY{n}{Audio}\PY{p}{(}\PY{n}{wave}\PY{p}{,} \PY{n}{rate}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{,} \PY{n}{autoplay}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
